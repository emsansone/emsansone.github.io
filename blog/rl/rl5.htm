<!DOCTYPE html>

<head>
<!-- saved from url=(0028) https://emsansone.github.io/ -->
<!-- <html class=" js no-touch rgba hsla textshadow opacity svg" lang="en"><!--<![endif]-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Emanuele Sansone</title>
<meta name="description" content="Emanuele Sansone">
<meta name="keywords" content="Jekyll, theme, responsive, blog, template">


<!-- Twitter Cards -->
<meta name="twitter:title" content="Emanuele Sansone">
<meta name="twitter:description" content="Emanuele Sansone">



<!--<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://emsansone.github.io/img/Background3.jpg">-->

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Emanuele Sansone">
<meta property="og:description" content="Emanuele Sansone">
<meta property="og:url" content="https://emsansone.github.io/blog/">
<meta property="og:site_name" content="Emanuele Sansone">





<link rel="canonical" href="https://emsansone.github.io/blog/">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="https://emsansone.github.io/assets/css/main.css">
    
<meta http-equiv="cleartype" content="on">
        
<!-- Modernizr -->
<script src="https://emsansone.github.io/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>
        
<link href='//fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700%7CPT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>


<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="https://emsansone.github.io/img/logo.png">
<!-- 32x32 -->
<link rel="shortcut icon" href="https://emsansone.github.io/img/logo-32x32.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="https://emsansone.github.io/img/logo-57x57.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://emsansone.github.io/img/logo-72x72.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://emsansone.github.io/img/logo-114x114.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://emsansone.github.io/img/logo-144x144.png">

<style>
  table, th, td {
    border: 1px solid grey;
    border-collapse: collapse;
  }
</style>

</head>

<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    // Center justify equations in code and markdown cells. Elsewhere
    // we use CSS to left justify single line equations in code cells.
    displayAlign: 'center',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}},
        linebreaks: { automatic: true }
    }
});
</script>

<body class="page">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="https://emsansone.github.io/">Website</a>
	</div><!-- /.site-name -->
    <div class="top-navigation">
        <nav role="navigation" id="site-nav" class="nav">
		    <ul>
              <li><a href="https://emsansone.github.io/blog/rl/rl0.htm">Index</a></li>	        					    
					    <li><a href="https://emsansone.github.io/blog/rl/rl4.htm" >Previous Lecture</a></li>
					    <li><a href="https://emsansone.github.io/blog/rl/rl6.htm" >Next Lecture</a></li>

		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->


<!--<div class="image-wrap">
    <img src="https://emsansone.github.io/img/Background3.jpg" alt="Emanuele Sansone feature image">
</div>-->

<div id="main" role="main">
  <div class="article-author-side">
    <img src="https://emsansone.github.io/img/Me.jpg" class="bio-photo" alt="Emanuele Sansone bio photo">

    <h2>Emanuele Sansone</h2>
    <p>PhD in machine learning and artificial intelligence.</p>
    <a href="mailto:e.sansone@hotmail.it" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/email.png" height=14px width=14px> Email</a>

    <a href="https://linkedin.com/in/emanuele-sansone-97329475" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/linkedin.png" height=14px width=14px> LinkedIn</a>

    <a href="https://twitter.com/skiera87" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/twitter.png" height=14px width=14px> Twitter</a>

    <!-- <a href="https://github.com/emsansone" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/github.png" height=14px width=14px> Github</a> -->
  </div>

  <article class="page">

    <div class="article-wrap" style="width:140%;text-align:justify;">
    
      <h1 id="Lecture-5:-Model-Free-Control">Model-Free Control</h1>
      <p><strong>Assumption:</strong></p>
      <ol>
      <li><strong>unknown MDP</strong>. You know the <strong>states</strong>, but you don't know the transition probabilities and the reward function</li>
      <li><strong>All states</strong> must be visited <strong>enough</strong> times.</li>
      </ol>
      <p><strong>Goal</strong>. We want to find the <strong>optimal policy</strong>.
      <br><strong>There are two main strategies, namely on-policy learning (learning by doing) and off-policy learning (learning from someone else's policy)</strong></p>
      
      <h1 id="On-Policy-Learning">On-Policy Learning</h1>
      <p>The main idea is to use <strong>Generalized Policy Iteration</strong>, which consists of iteratively applying two steps, viz. policy evaluation (computing the value of a policy) and policy improvement.</p><br/>
      <p style="text-align:center;"><img src="./img/Generalised.png" alt="Generalized" style="width:50%;height:50%;"></p>

      <h2 id="On-Policy---Trial-1:-Generalized-Policy-Iteration-with-Monte-Carlo-Evaluation">On-Policy - Trial 1: Generalized Policy Iteration with Monte-Carlo Evaluation</h2>
      <blockquote><p><strong>Policy Evaluation</strong> Monte-Carlo policy evaluation on <strong>state-value function</strong>.
      <br><strong>Policy Improvement</strong> Greedy policy improvement $\pi(s)=\arg\max_a R_s^a + \gamma\sum_{s'}P_{ss'}^a v(s')$.</p>
      </blockquote>
      <p><strong>Problem 1</strong>. This solution is <strong>not model-free</strong>. You need $P$.
      <br><strong>Problem 2</strong>. Deterministic policy has issues about <strong>exploration</strong>.</p>

      <h2 id="On-Policy---Trial-2:-Generalized-Policy-Iteration-with-Action-Value-Function">On-Policy - Trial 2: Generalized Policy Iteration with Action-Value Function</h2>
      <p>If you use the action-value function, you can do control without needing the model</p>
      <blockquote><p><strong>Policy Evaluation</strong> Monte-Carlo policy evaluation on <strong>action-value function</strong>.
      <br><strong>Policy Improvement</strong> Greedy policy improvement $\pi(s)=\arg\max_a q(a,s)$.</p>
      </blockquote>
      <p><strong>Problem 2</strong>. Deterministic policy has issues about <strong>exploration</strong>.
      <br>Consider a case with two possible actions. One of them gives small reward at each timestep, while the other one gives huge and rare rewards.
      A greedy policy always selects the action with small and frequent reward, but it's not guaranteed to be optimal.</p>

      <h2 id="On-policy---Trial-3:-Monte-Carlo-Policy-Iteration">On-policy - Trial 3: Monte-Carlo Policy Iteration</h2>
      <blockquote><p><strong>Policy Evaluation</strong> Monte-Carlo policy evaluation on <strong>action-value function</strong>.
      <br><strong>Policy Improvement</strong> $\epsilon$-Greedy exploration, namely choose greedy action with probability $1-\epsilon$ or choose all other actions with probability $\epsilon$. Therefore</p>
      $$\pi'(a|s)=\left\{\begin{array}{ll} 
                          1-\epsilon+\frac{\epsilon}{m} &amp; if a=\arg\max_{a'}q_\pi(a',s) \\
                          \frac{\epsilon}{m} &amp; otherwise
      \end{array}\right.$$
      </blockquote>
      <p><strong>Theorem</strong>. $\epsilon$-greedy exploration improves the policy, namely $v_{\pi'}(s)\geq v_\pi(s)$.
      <br><strong>Proof</strong>. $$\begin{align}
                   \sum_{a}\pi'(a|s)q_\pi(a,s) &amp;= (1-\epsilon)\max_a q_\pi(a,s) + \frac{\epsilon}{m}\sum_a q_\pi(a,s) \\
                                               &amp;\geq (1-\epsilon)\sum_a \pi^{''}(a|s) q_\pi(a,s) + \frac{\epsilon}{m}\sum_a q_\pi(a,s) \quad\text{where}\quad \pi^{''}(a|s)=\frac{\pi(a|s)-\epsilon/m}{1-\epsilon} \\
                                               &amp;= (1-\epsilon)\sum_a \frac{\pi(a|s)-\epsilon/m}{1-\epsilon}q_\pi(a,s) + \frac{\epsilon}{m}\sum_a q_\pi(a,s)\\
                                               &amp;= \sum_{a}\pi(a|s)q_\pi(a,s)
                  \end{align}$$
      <strong>QED</strong></p><br/>
      <p><strong>Problem</strong>. This is a bit inefficient, because we evaluate a policy on multiple episodes. If the policy is bad, we spend too much time on evaluating that bad policy.</p>

      <h2 id="On-policy---Trial-4:-Monte-Carlo-Control">On-policy - Trial 4: Monte-Carlo Control</h2>
      <p>Same as Monte-Carlo Policy Iteration but <strong>you evaluate the policy on just a single episode</strong>.</p>
      <blockquote><p><strong>Policy Evaluation</strong> Monte-Carlo policy evaluation on <strong>action-value function</strong> on <strong>single episode</strong>. $q(a,s)\approx q_\pi(a,s)$.
      <br><strong>Policy Improvement</strong> $\epsilon$-Greedy exploration, namely choose greedy action with probability $1-\epsilon$ or choose all other actions with probability $\epsilon$. Therefore</p>
      $$\pi'(a|s)=\left\{\begin{array}{ll} 
                          1-\epsilon+\frac{\epsilon}{m} &amp; if a=\arg\max_{a'}q(a',s) \\
                          \frac{\epsilon}{m} &amp; otherwise
      \end{array}\right.$$
      </blockquote>
      <p><strong>Problem</strong>. Note that policy evaluation is performed on a single episode (and not an infinite set of episodes), which obtains only an approximation of the action-value function. The subsequent policy improvement is based on this estimation. Are we guaranteed to find the <strong>optimal policy</strong>?</p>
      
      <h2 id="On-policy---Solution-1:-GLIE-Monte-Carlo-Control">On-policy - Solution 1: GLIE Monte-Carlo Control</h2>
      <p>In order to ensure convergence to the optimal policy, we need to satisfy two conditions:</p>
      <blockquote><p><strong>GLIE (Greedy in the Limit of Infinite Exploration)</strong>.</p>
      <ol>
      <li>All state-action pairs must be explored infinitely many times (in order not to discard any of them), namely $\lim_{t\rightarrow\infty}N_t(a,s)=\infty$.</li>
      <li>The final policy must be greedy (deterministic), $\lim_{t\rightarrow\infty}\pi_t(a|s)=1[a=\arg\max_{a'}q_t(a',s)]$.</li>
      </ol>
      </blockquote>
      <p>Note that the GLIE property can be ensure by choosing $\epsilon=1/t$.</p>
      <blockquote><p><strong>Policy Evaluation</strong> Monte-Carlo policy evaluation on <strong>action-value function</strong> on <strong>single episode</strong>.</p>
      $$\begin{align}\text{for each pair of state,} &amp; \text{ action in the episode} \\
                           &amp; N(A_t,S_t)=N(A_t,S_t)+1 \\
                           &amp; q(S_t,A_t)=q(S_t,A_t) + \frac{1}{N(A_t,S_t)}\big(G_t-q(S_t,A_t)\big)\end{align}$$<br/>
      <p><strong>Policy Improvement</strong> $\epsilon$-Greedy exploration, namely choose greedy action with probability $1-\epsilon$ or choose all other actions with probability $\epsilon$. Therefore</p>
      $$\pi'(a|s)=\left\{\begin{array}{ll} 
                          1-\epsilon+\frac{\epsilon}{m} &amp; if a=\arg\max_{a'}q(a',s) \\
                          \frac{\epsilon}{m} &amp; otherwise
      \end{array}\right.$$<br/>
      <p></p>Choose $\epsilon=1/t$.</p>
      </blockquote>
      <p><strong>Theorem</strong>. GLIE Monte-Carlo control converges to the optimal action-value function as long as the number of collected episodes is large enough, namely $q(A_t,S_t)\rightarrow q^*(A_t,S_t)$ for $t\rightarrow\infty$.
      <br><strong>Proof</strong>. NOT HERE</p>

      <h2 id="On-policy---Solution-2:-Control-with-Sarsa">On-policy - Solution 2: Control with Sarsa</h2>
      <p>We want to have an <strong>online</strong> algorithm.
      <br>Recall that MC policy evaluation uses $q(S_t,A_t)=q(S_t,A_t) + \alpha\big(G_t-q(S_t,A_t)\big)$.
      Then, we can replace $G_t$ with the TD target $R_{t+1}+\gamma q(S_{t+1},A_{t+1})$ and obtain Sarsa.
      (In fact, recall that $q_\pi(a,s)=R_s^a+\gamma\sum_{s'}P_{ss'}^a\sum_{a'}\pi(a'|s')q_\pi(a',s')$).</p>
      <blockquote><p><strong>Policy Evaluation</strong> <strong>online</strong> policy evaluation on <strong>action-value function</strong>.</p>
      $$q(S_t,A_t)=q(S_t,A_t) + \alpha\big(R_{t+1}+\gamma q(S_{t+1},A_{t+1})-q(S_t,A_t)\big)$$<br/>
      <p><strong>Policy Improvement</strong> $\epsilon$-Greedy exploration, namely choose greedy action with probability $1-\epsilon$ or choose all other actions with probability $\epsilon$. Therefore
      $$\pi'(a|s)=\left\{\begin{array}{ll} 
                          1-\epsilon+\frac{\epsilon}{m} &amp; if a=\arg\max_{a'}q(a',s) \\
                          \frac{\epsilon}{m} &amp; otherwise
      \end{array}\right.$$
      Choose $\epsilon=1/t$.</p>
      </blockquote>
      <p style="text-align:center;"><img src="./img/Sarsa.png" alt="Sarsa" style="width:10%;height:10%;"></p><br/>
      <p><strong>Theorem</strong>. Assume that
      <ol>
      <li>We have GLIE policies</li>
      <li>The sequence of $\alpha_t$ satisfies the Robbins Monro conditions:<ol>
      <li>$\sum_{t=1}^\infty \alpha_t = \infty$ (meaning that we are able to move our estimate $q(s,a)$ wherever we want).</li>
      <li>$\sum_{t=1}^\infty\alpha_t^2&lt;\infty$ (meaning that the changes eventually diminish).</li>
      </ol>
      </li>
      </ol>
      <br>Then, Sarsa converges to optimal action-value function.
      <br><strong>Proof</strong>. NOT HERE
      <br><strong>Question</strong>. Can we unify the last two solutions?</p>

      <h2 id="On-policy---Solution-3:-Control-with-n-Step-Sarsa">On-policy - Solution 3: Control with n-Step Sarsa</h2>
      <p>We can replace $G_t$ with the n-step TD target $q_t^{(n)}=R_{t+1}+\gamma R_{t+2} + \dots + \gamma^{n}q(S_{t+n},A_{t+n})$</p>
      <blockquote><p><strong>Policy Evaluation</strong> policy evaluation on <strong>action-value function</strong></p>
      $$q(S_t,A_t)=q(S_t,A_t) + \alpha\big(q_t^{(n)}-q(S_t,A_t)\big)$$<br/>
      <p><strong>Policy Improvement</strong> $\epsilon$-Greedy exploration, namely choose greedy action with probability $1-\epsilon$ or choose all other actions with probability $\epsilon$. Therefore</p>
      $$\pi'(a|s)=\left\{\begin{array}{ll} 
                          1-\epsilon+\frac{\epsilon}{m} &amp; if a=\arg\max_{a'}q(a',s) \\
                          \frac{\epsilon}{m} &amp; otherwise
      \end{array}\right.$$
      <p>Choose $\epsilon=1/t$.</p>
      </blockquote>
      <p><strong>Question</strong>. Which $n$ is the best?</p>

      <h2 id="On-policy---Solution-4:-Control-with-Sarsa($\lambda$)">On-policy - Solution 4: Control with Sarsa($\lambda$)</h2>
      <p>Combine all $n$</p>
      <blockquote><p><strong>Policy Evaluation</strong> policy evaluation on <strong>action-value function</strong>.</p>
      $$q(S_t,A_t)=q(S_t,A_t) + \alpha\big(q_t^{\lambda}-q(S_t,A_t)\big)$$<br/>
      where $q_t^{\lambda}=(1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}q_t^{(n)}$.
      <p><strong>Policy Improvement</strong> $\epsilon$-Greedy exploration, namely choose greedy action with probability $1-\epsilon$ or choose all other actions with probability $\epsilon$. Therefore</p>
      $$\pi'(a|s)=\left\{\begin{array}{ll} 
                          1-\epsilon+\frac{\epsilon}{m} &amp; if a=\arg\max_{a'}q(a',s) \\
                          \frac{\epsilon}{m} &amp; otherwise
      \end{array}\right.$$<br/>
      <p>Choose $\epsilon=1/t$.</p>
      </blockquote>
      <p><strong>Question</strong>. How to make it online?</p>

      <h2 id="On-policy---Solution-5:-Control-with-Online-Sarsa($\lambda$)">On-policy - Solution 5: Control with Online Sarsa($\lambda$)</h2>
      <p>Define an eligibility trace, namely:</p>
      $$E_0(a,s)=0$$<br/>
      $$E_t(a,s)=\gamma\lambda E_{t-1}(a,s) + 1[S_t=s, A_t=a]$$<br/>
      <p>Then,</p>
      <blockquote><p><strong>Policy Evaluation</strong> policy evaluation on <strong>action-value function</strong>.</p>
      $$q(S_t,A_t)=q(S_t,A_t) + \alpha\delta_tE_t(S_t,A_t)$$<br/>
      <p>where $\delta_t=R_{t+1}+\gamma q(S_{t+1},A_{t+1})-q(S_t,A_t)$.
      <br><strong>Policy Improvement</strong> $\epsilon$-Greedy exploration, namely choose greedy action with probability $1-\epsilon$ or choose all other actions with probability $\epsilon$. Therefore</p>
      $$\pi'(a|s)=\left\{\begin{array}{ll} 
                          1-\epsilon+\frac{\epsilon}{m} &amp; if a=\arg\max_{a'}q(a',s) \\
                          \frac{\epsilon}{m} &amp; otherwise
      \end{array}\right.$$<br/>
      <p>Choose $\epsilon=1/t$.</p>
      </blockquote>

      <h1 id="Off-Policy-Learning">Off-Policy Learning</h1>
      <p>We have a <strong>behaviour policy</strong> $\mu$ (the one which really acts in the environment) and a <strong>target policy</strong> $\pi$, which we want to know its state-value function.
      <br>Why?</p>
      <ol>
      <li>The behaviour policy can be from other agents (e.g. humans)</li>
      <li>Re-use experience from old policies</li>
      <li>Following exploratory policy and learn the optimal one</li>
      <li>Follow one policy and learn multiple ones</li>
      </ol>
      <blockquote><p><strong>Importance sampling</strong>. Imagine you have 2 distributions $P$ and $Q$. Then</p>
      $$\begin{align}E_{X\sim P}\{f(X)\} &amp;= \sum P(X)f(X) \\
                                         &amp;= \sum P(X)\frac{Q(X)}{Q(X)}f(X) \\
                                         &amp;= \sum Q(X)\bigg(\frac{P(X)}{Q(X)}f(x)\bigg) \\
                                         &amp;= E_{X\sim Q}\bigg\{\frac{P(X)}{Q(X)}f(x)\bigg\}\end{align}$$<br/>
      </blockquote>
      <p><strong>The quantity we want to estimate is $G_t$</strong>.</p>

      <h2 id="Trial-1:-Off-Policy-Monte-Carlo">Trial 1: Off-Policy Monte Carlo</h2>
      <p>We take actions using $\mu$ and we estimate $G_t$ by <strong>importance sampling over the actions</strong>.</p>
      $$\begin{align}G_t^{\pi/\mu} &= \frac{\pi(A_t,A_{t+1},\dots,A_T|S_t,S_{t+1},\dots,S_T)}{\mu(A_t,A_{t+1},\dots,A_T|S_t,S_{t+1},\dots,S_T)}G_t \\ 
                                   &=\frac{\pi(A_t|S_t,S_{t+1},\dots,S_T)\pi(A_{t+1}|S_t,S_{t+1},\dots,S_T)\dots\pi(A_T|S_t,S_{t+1},\dots,S_T)}{\mu(A_t|S_t,S_{t+1},\dots,S_T)\mu(A_{t+1}|S_t,S_{t+1},\dots,S_T)\dots\mu(A_T|S_t,S_{t+1},\dots,S_T)}G_t \\
                                   &=\frac{\pi(A_t|S_t)\pi(A_{t+1}|S_{t+1})\dots\pi(A_T|S_T)}{\mu(A_t|S_t)\mu(A_{t+1}|S_{t+1})\dots\mu(A_T|S_T)}G_t\end{align}$$<br/>
      <p>and the update is</p>
      $$v(S_t)=v(S_t)+\alpha\big(G_t^{\pi/\mu}-v(S_t)\big)$$<br/>
      <p><strong>The main problem with this algorithm is that the estimator has too high variance!</strong>
      <br><strong>In practice, Monte Carlo is never used for off-policy learning</strong>.</p>

      <h2 id="Solution-1:-Off-Policy-Temporal-Difference">Solution 1: Off-Policy Temporal Difference</h2>
      <p>We take actions using $\mu$ and we estimate $G_t$ by <strong>importance sampling over the actions</strong>.</p>
      $$G_t^{\pi/\mu}=\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}\big(R_{t+1}+\gamma v(S_{t+1})\big)$$<br/>
      <p>and the update is</p>
      $$v(S_t)=v(S_t)+\alpha\big(G_t^{\pi/\mu}-v(S_t)\big)$$<br/>
      <p><strong>This solution has much less variance compared to off-policy MC</strong>.</p>

      <h2 id="Solution-2:-General-Q-Learning">Solution 2: General Q-Learning</h2>
      <p><strong>We don't need to use importance sampling</strong>. In fact, we can use a <strong>behaviour policy $\mu$</strong> to act in the environment.
      And we update the TD target using an alternative action from <strong>target policy</strong> $\pi$.</p>
      $$q(S_t,A_t)=q(S_t,A_t) + \alpha\big(R_{t+1}+\gamma q(S_{t+1},A_{t+1})-q(S_t,A_t)\big)$$<br/>
      <p>where $A_{t+1}=\arg \max_a q(S_{t+1},a)$ (from policy $\pi$). Therefore,</p>
      $$q(S_t,A_t)=q(S_t,A_t) + \alpha\big(R_{t+1}+\gamma \max_a q(S_{t+1},a)-q(S_t,A_t)\big)$$<br/>

      <h2 id="Solution-3:-Special-case-of-Q-Learning-(Sarsa-MAX)">Solution 3: Special case of Q-Learning (Sarsa MAX)</h2>
      <p>Here, we have a special case of Q-Learning. <strong>Case where we want learn a greedy behaviour following an explorative behaviour</strong>.
        In fact, <strong>behaviour policy $\mu$</strong> is <strong>$\epsilon$-greedy</strong>, while the <strong>target policy</strong> $\pi$ is <strong>greedy</strong>.</p>
      <blockquote><p><strong>Policy Evaluation</strong> policy evaluation on <strong>action-value function</strong>.</p>
      $$q(S_t,A_t)=q(S_t,A_t) + \alpha\big(R_{t+1}+\gamma \max_a q(S_{t+1},a)-q(S_t,A_t)\big)$$<br/>
      <p><strong>Policy Improvement</strong> $\epsilon$-Greedy exploration, namely choose greedy action with probability $1-\epsilon$ or choose all other actions with probability $\epsilon$. Therefore</p>
      $$\mu'(a|s)=\left\{\begin{array}{ll} 
                          1-\epsilon+\frac{\epsilon}{m} &amp; if a=\arg\max_{a'}q(a',s) \\
                          \frac{\epsilon}{m} &amp; otherwise
      \end{array}\right.$$<br/>
      </blockquote>
      <p><strong>Theorem</strong>. Sarsa MAX converges to optimal action-value function. (In fact, it fulfills the GLIE property)
      <br><strong>Proof</strong>. NOT HERE</p>
     
    
    </div><!-- /.article-wrap -->

    <div id="disqus_thread" style="width:140%;text-align:justify;margin-top:3cm;margin-bottom:2cm;"></div>
    <script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://ema87.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
    </script>
            
  </article>
</div><!-- /#index -->
                    

<div class="footer-wrap">
  <footer>
<span>&copy; 2020 Emanuele Sansone.

  </span></footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="https://emsansone.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="https://emsansone.github.io/assets/js/scripts.min.js"></script>

</body></html>
