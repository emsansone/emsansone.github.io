<!DOCTYPE html>

<head>
<!-- saved from url=(0028) https://emsansone.github.io/ -->
<!-- <html class=" js no-touch rgba hsla textshadow opacity svg" lang="en"><!--<![endif]-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Emanuele Sansone</title>
<meta name="description" content="Emanuele Sansone">
<meta name="keywords" content="Jekyll, theme, responsive, blog, template">


<!-- Twitter Cards -->
<meta name="twitter:title" content="Emanuele Sansone">
<meta name="twitter:description" content="Emanuele Sansone">



<!--<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://emsansone.github.io/img/Background3.jpg">-->

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Emanuele Sansone">
<meta property="og:description" content="Emanuele Sansone">
<meta property="og:url" content="https://emsansone.github.io/blog/">
<meta property="og:site_name" content="Emanuele Sansone">





<link rel="canonical" href="https://emsansone.github.io/blog/">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="https://emsansone.github.io/assets/css/main.css">
    
<meta http-equiv="cleartype" content="on">
        
<!-- Modernizr -->
<script src="https://emsansone.github.io/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>
        
<link href='//fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700%7CPT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>


<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="https://emsansone.github.io/img/logo.png">
<!-- 32x32 -->
<link rel="shortcut icon" href="https://emsansone.github.io/img/logo-32x32.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="https://emsansone.github.io/img/logo-57x57.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://emsansone.github.io/img/logo-72x72.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://emsansone.github.io/img/logo-114x114.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://emsansone.github.io/img/logo-144x144.png">

</head>


<!-- Loading mathjax macro -->
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    // Center justify equations in code and markdown cells. Elsewhere
    // we use CSS to left justify single line equations in code cells.
    displayAlign: 'center',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}},
        linebreaks: { automatic: true }
    }
});
</script>
<!-- End of mathjax configuration --></head>

<body class="page">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="https://emsansone.github.io/blog/rl/rl0.htm">General</a>
	</div><!-- /.site-name -->
    <div class="top-navigation">
        <nav role="navigation" id="site-nav" class="nav">
		    <ul>
		        					    
					    <li><a href="https://emsansone.github.io/blog/rl/rl2.htm" >Next Lecture</a></li>

		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->


<!--<div class="image-wrap">
    <img src="https://emsansone.github.io/img/Background3.jpg" alt="Emanuele Sansone feature image">
</div>-->

<div id="main" role="main">
  <div class="article-author-side" style="width:10%">
    <img src="https://emsansone.github.io/img/Me.jpg" class="bio-photo" alt="Emanuele Sansone bio photo">

    <h2>Emanuele Sansone</h2>
    <p>PhD in machine learning and artificial intelligence.</p>
    <a href="mailto:e.sansone@hotmail.it" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/email.png" height=14px width=14px> Email</a>

    <a href="https://linkedin.com/in/emanuele-sansone-97329475" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/linkedin.png" height=14px width=14px> LinkedIn</a>

    <a href="https://twitter.com/skiera87" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/twitter.png" height=14px width=14px> Twitter</a>

    <!-- <a href="https://github.com/emsansone" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/github.png" height=14px width=14px> Github</a> -->
  </div>

  <article class="page">

    <div class="article-wrap" style="width:140%;text-align:justify;margin-bottom:2cm;">
    
      <h1 id="Introduction-to-Reinforcement-Learning">Introduction to Reinforcement Learning</h1>
      <p>Reinforcement Learning (RL) is a <strong>paradigm</strong> of learning from data (an area of machine learning).</p>
      <p>In RL, there are two main components, namely an <strong>agent</strong> and an <strong>environment</strong>.
      These two components <strong>interact</strong> iteratively over time. On one hand, the environment generates an <strong>observation</strong> 
      $O_t$ of its state and a <strong>reward</strong> $R_t$, given an action $A_{t-1}$ chosen by the agent. On the other hand, the agent receives 
      the observation and the reward from the environment and chooses a new <strong>action</strong> $A_t$. The following picture summarizes the RL paradigm.</p>
      <p style="text-align:center;"><img src="./img/RLmodel.png" alt="Rl model" style="width:40%;height:40%;"></p>
      <p>RL is general enough to capture the notion of machine intelligence (see this <a href="https://arxiv.org/pdf/0712.3329.pdf">definition</a>). It 
        is more general than traditional <strong>supervised learning</strong>, as (i) the supervisory signal (i.e. the reward) is not directly linked to 
        the agent output, (ii) there is no i.i.d. data assumption and (iii) agent actions may affect subsequent data. Thanks to these properties, it is 
        used in <strong>a large number of real-world problems</strong> involving decision making.</p>

      <h2 id="Definitions">Definitions</h2><blockquote>
      <br><strong>Reward</strong>. There are two kinds of reward, namely the external reward provided by the environment and the intrinsic reward 
      created by the agent. The external reward comes in many different forms</br>
      <br><strong>History</strong>. $H_t=O_1,R_1,\dots,A_{t-1},O_t,R_t$</br>
      <br><strong>State</strong>. A state is a function of the entire history, i.e. $S_t=f(H_t)$. We have the <strong>environment state</strong> and 
      the <strong>agent state</strong>.</br>
      </blockquote>
      <p>Note that a state is <strong>Markov</strong> iff $p(S_{t+1}|S_t)=p(S_{t+1}|S_t,\dots,S_1)$.
      (if $f(H_t)=H_t$, then it's obvious that the state is Markov. The environment state is by definition a Markov state.)</p>

      <h2 id="The-environment">The environment</h2>
      <p>The environment performs two actions:</p>
      <ol>
      <li><strong>Take action</strong>. Given $A_{t-1}$ and $S^e_{t-1}$ compute $R_t$ and $O_t$.</li>
      <li><strong>Update state</strong>. Given $A_{t-1},O_t,R_t$ and $S^e_{t-1}$ compute $S^e_t$.</li>
      </ol>
      <p>The environment can be</p>
      <ol>
      <li><strong>Fully observable</strong>. Meaning that $O_t=S^e_t$. Therefore, we have a <strong>Markov Decision Process</strong></li>
      <li><strong>Partially observable</strong>. Meaning that $O_t\neq S^e_t$. Therefore, we have a <strong>Partially Observable Markov Decision 
        Process</strong></li>
      </ol>

      <h2 id="The-agent">The agent</h2>
      <p>The environment performs two actions:</p>
      <ol>
      <li><strong>Update state</strong>. Given $O_t,R_t$ and $S^a_{t-1}$ compute $S^a_t$.</li>
      <li><strong>Take action</strong>. Given $S^a_t$ compute $A_t$.</li>
      </ol>
      <p>In MDP, a good choice for $S^a_t\doteq O_t$.
      In POMDP, there are different choices:</p>
      <ol>
      <li>Complete history, $S^a_t=H_t$.</li>
      <li>Beliefs of the environment, $S^a_t=(P(S^e_t=s_1),\dots,P(S^e_t=s_n))$.</li>
      <li>Recurrent relation, $S^a_t=\sigma(W_sS^a_{t-1}+W_oO_t)$.</li>
      </ol>
      <p>The agent can have some of the following components:</p>
      <ol>
      <li><strong>Policy</strong>.</li>
      <li><strong>Value function</strong>.</li>
      <li><strong>Model</strong>.</li>
      </ol>

      <h3 id="Policy">Policy</h3>
      <p>It defines the <strong>agent's behaviour</strong>. It can be deterministic, viz. $A_t=\pi(S^a_t)$.
      Or it can be stochastic, viz. $\pi(a|s)=P(A_t=a|S^a_t=s)$.</p>

      <h3 id="Value-function">Value function</h3>
      <p>Prediction of future reward for a given policy $\pi$.
      For example, $v_\pi(s)=E_\pi\{R_{t+1}+\gamma R_{t+2}+ \gamma^2 R_{t+3} + \dots|S_t=s\}$.</p>

      <h3 id="Model">Model</h3>
      <p>It predicts what the environment will do next.
      Typically, you learn a transition model and a reward model.</p>

      <h2 id="Taxonomy-of-RL-agent">Taxonomy of RL agent</h2>
      <p><img src="./img/Taxonomy.png" alt="Taxonomy"></p>

      <h2 id="Problems-within-RL">Problems within RL</h2>
      <p>The environment is initially unknown and the agent must learn through interaction.
      One way to do reinforcement learning is to first learn the model of the environment, and then plan.
      Therefore, <strong>the planning problem</strong> starts from the assumption that the model is known.</p>
      <p>Two main problems:</p>
      <ol>
      <li><strong>Prediction</strong>. Given a policy, compute the value function</li>
      <li><strong>Control</strong>. Find the best policy</li>
      </ol>
      
    </div><!-- /.article-wrap -->

    <hr>

    <div id="disqus_thread" style="width:140%;text-align:justify;margin-top:2cm;margin-bottom:2cm;"></div>
    <script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://ema87.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
    </script>
            
  </article>
</div><!-- /#index -->
                    

<div class="footer-wrap">
  <footer>
<span>&copy; 2020 Emanuele Sansone.

  </span></footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="https://emsansone.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="https://emsansone.github.io/assets/js/scripts.min.js"></script>

</body></html>
