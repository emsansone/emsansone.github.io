<!DOCTYPE html>

<head>
<!-- saved from url=(0028) https://emsansone.github.io/ -->
<!-- <html class=" js no-touch rgba hsla textshadow opacity svg" lang="en"><!--<![endif]-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Emanuele Sansone</title>
<meta name="description" content="Emanuele Sansone">
<meta name="keywords" content="Jekyll, theme, responsive, blog, template">


<!-- Twitter Cards -->
<meta name="twitter:title" content="Emanuele Sansone">
<meta name="twitter:description" content="Emanuele Sansone">



<!--<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://emsansone.github.io/img/Background3.jpg">-->

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Emanuele Sansone">
<meta property="og:description" content="Emanuele Sansone">
<meta property="og:url" content="https://emsansone.github.io/blog/">
<meta property="og:site_name" content="Emanuele Sansone">





<link rel="canonical" href="https://emsansone.github.io/blog/">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="https://emsansone.github.io/assets/css/main.css">
    
<meta http-equiv="cleartype" content="on">
        
<!-- Modernizr -->
<script src="https://emsansone.github.io/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>
        
<link href='//fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700%7CPT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>


<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="https://emsansone.github.io/img/logo.png">
<!-- 32x32 -->
<link rel="shortcut icon" href="https://emsansone.github.io/img/logo-32x32.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="https://emsansone.github.io/img/logo-57x57.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://emsansone.github.io/img/logo-72x72.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://emsansone.github.io/img/logo-114x114.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://emsansone.github.io/img/logo-144x144.png">

<style>
  table, th, td {
    border: 1px solid grey;
    border-collapse: collapse;
  }
</style>

</head>

<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    // Center justify equations in code and markdown cells. Elsewhere
    // we use CSS to left justify single line equations in code cells.
    displayAlign: 'center',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}},
        linebreaks: { automatic: true }
    }
});
</script>

<body class="page">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="https://emsansone.github.io/">Website</a>
	</div><!-- /.site-name -->
    <div class="top-navigation">
        <nav role="navigation" id="site-nav" class="nav">
		    <ul>
              <li><a href="https://emsansone.github.io/blog/rl/rl0.htm">Index</a></li>	        					    
					    <li><a href="https://emsansone.github.io/blog/rl/rl4.htm" >Previous Lecture</a></li>
					    <li><a href="https://emsansone.github.io/blog/rl/rl6.htm" >Next Lecture</a></li>

		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->


<!--<div class="image-wrap">
    <img src="https://emsansone.github.io/img/Background3.jpg" alt="Emanuele Sansone feature image">
</div>-->

<div id="main" role="main">
  <div class="article-author-side">
    <img src="https://emsansone.github.io/img/Me.jpg" class="bio-photo" alt="Emanuele Sansone bio photo">

    <h2>Emanuele Sansone</h2>
    <p>PhD in machine learning and artificial intelligence.</p>
    <a href="mailto:e.sansone@hotmail.it" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/email.png" height=14px width=14px> Email</a>

    <a href="https://linkedin.com/in/emanuele-sansone-97329475" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/linkedin.png" height=14px width=14px> LinkedIn</a>

    <a href="https://twitter.com/skiera87" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/twitter.png" height=14px width=14px> Twitter</a>

    <!-- <a href="https://github.com/emsansone" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/github.png" height=14px width=14px> Github</a> -->
  </div>

  <article class="page">

    <div class="article-wrap" style="width:140%;text-align:justify;">
    
      <h1 id="Lecture-6:-Value-Function-Approximation">Value Function Approximation</h1>
      <p>Note that in many interesting real-world applications the number of states is very large.
      For example, in backgammon the number of states is in the order of $10^{20}$, in Computer Go is in the order of $10^{170}$ and in helicopter we have a continuous state space and therefore the number is uncountably infinite.
      <br>In such scenarios, we have <strong>two problems</strong>:
      <ol>
      <li><strong>Memory</strong>. In fact, it could be not possible to store the values for each state or for each actio-state pair.</li>
      <li><strong>Inefficient Learning</strong>. It is very inefficient to learn the value of each state independently (<strong>they are not independent</strong>).</li>
      </ol>
      <strong>Solution</strong>. Use a <strong>function approximator</strong>. For each state or action-state pair, you can compute the corresponding value <strong>without storing it</strong>. Furthermore, the <strong>parameters</strong> of the function approximator are in general much less than the number of states, this allows for capturing the <strong>dependence between states</strong> and <strong>generalize to unseen states</strong>.</p>
      $$\hat{v}(s;w)\approx v_\pi(s)$$<br/>
      $$\hat{q}(a,s;w)\approx q_\pi(a,s)$$<br/>
      <p>There are <strong>three kinds of function approximator</strong>.</p><br/>
      <p style="text-align:center"><img src="./img/function.png" alt="Function" style="width:50%;height:50%;"></p><br/>
      <p><strong>Question</strong>. How can we learn such function approximators?
      <br><strong>Answer</strong>. Through <strong>supervised learning</strong>.
      <br>There are mainly two categories, namely <strong>incremental</strong> methods and <strong>batch</strong> methods.
      Incremental methods use samples, perform updates and throw away them, whereas batch methods store samples and reuse them.</p>

      <h1 id="Incremental-methods:-Prediction">Incremental methods: Prediction</h1>
      <p>Imagine that for a policy $\pi$, we can access a <strong>sequence of training pairs</strong>, namely:</p>
      $$\big(S_1,v_\pi(S_1)\big),\big(S_2,v_\pi(S_2)\big),\dots,\big(S_t,v_\pi(S_t)\big),\dots$$<br/>
      <p>Now, for a specific state $s$, we can compute the following ojective, called <strong>Mean Squared Value Error (MSVE)</strong></p>
      $$\begin{align}J(w)&amp;=\frac{1}{2}E\big\{\big(v_\pi(s)-\hat{v}(s;w)\big)^2\big\} \\
                         &amp;=\frac{1}{2}\sum_s p(s)E\big\{\big(v_\pi(S_t)-\hat{v}(S_t;w)\big)^2|S_t=s\big\}\end{align}$$<br/>
      <p>Then, we can learn by SGD</p>
      $$\begin{align}\Delta w &amp;= \alpha E\big\{\big(v_\pi(s)-\hat{v}(s;w)\big)\nabla_w \hat{v}(s;w)\big\} \\
                              &amp;\approx \alpha\big(v_\pi(s)-\hat{v}(s;w)\big)\nabla_w \hat{v}(s;w)\end{align}$$<br/>
      <p>where the last step is computed using a <strong>finite sequence of training pairs</strong> (an episode of policy $\pi$).
      <br><strong>Problem</strong>. This sequence is not available in practice as $v_\pi(\cdot)$ is not known.</p>
      
      <h2 id="Solution-1:-Monte-Carlo-with-Function-Approximation">Solution 1: Monte Carlo with Function Approximation</h2>
      <p>Now the sequence is</p>
      $$\big(S_1,G_1\big),\big(S_2,G_2\big),\dots,\big(S_t,G_t\big),\dots,\big(S_t,G_T\big)$$<br/>
      <p>and the update is given by</p>
      <blockquote><p>$$\begin{align}\Delta w &amp;\approx \alpha\big(G_t-\hat{v}(s;w)\big)\nabla_w \hat{v}(s;w)\end{align}$$</p>
      </blockquote>
      <p><strong>Theorem</strong>. Monte Carlo with Function Approximation finds an unbiased estimate of the true state-value function.
      <br><strong>Proof</strong>. The objective for Monte Carlo with Function Approximation is given by the <strong>Mean Squared Return Error (MSRE)</strong>, namely:</p>
      $$\begin{align}J(w)&amp;=E\big\{\big(G_t-\hat{v}(s;w)\big)^2\big\} \\
                         &amp;=\sum_s p(s)E\big\{\big(G_t-\hat{v}(S_t;w)\big)^2|S_t=s\big\} \\
                         &amp;=\sum_s p(s)E\big\{\big(G_t-v_\pi(S_t)-\text{error}(S_t;w)\big)^2|S_t=s\big\} \\
                         &amp;=\sum_s p(s)E\big\{\big(G_t-v_\pi(S_t)\big)^2-2\big(G_t-v_\pi(S_t)\big)\text{error}(S_t;w)+\text{error}(S_t;w)^2|S_t=s\big\} \\
                         &amp;=\sum_s p(s)\bigg[E\big\{\big(G_t-v_\pi(S_t)\big)^2|S_t=s\big\}-2E\big\{\big(G_t-v_\pi(S_t)\big)\text{error}(S_t;w)|S_t=s\big\}+E\big\{\text{error}(S_t;w)^2|S_t=s\big\}\bigg] \\
                         &amp;=\sum_s p(s)\bigg[E\big\{\big(G_t-v_\pi(S_t)\big)^2|S_t=s\big\}-2E\big\{\big(G_t-v_\pi(S_t)\big)|S_t=s\big\}E\big\{\text{error}(S_t;w)|S_t=s\big\}+E\big\{\text{error}(S_t;w)^2|S_t=s\big\}\bigg] \\
                         &amp;=\sum_s p(s)\bigg[E\big\{\big(G_t-v_\pi(S_t)\big)^2|S_t=s\big\}+E\big\{\text{error}(S_t;w)^2|S_t=s\big\}\bigg] \\
                         &amp;=\sum_s p(s)\bigg[E\big\{\big(G_t-v_\pi(S_t)\big)^2|S_t=s\big\}+E\big\{\big(v_\pi(S_t)-\hat{v}(S_t;w)\big)^2|S_t=s\big\}\bigg] \\
                         &amp;=\sum_s p(s)\bigg[E\big\{\big(G_t-v_\pi(S_t)\big)^2|S_t=s\big\}\bigg]+\sum_s p(s)\bigg[E\big\{\big(v_\pi(S_t)-\hat{v}(S_t;w)\big)^2|S_t=s\big\}\bigg] \\
                         &amp;=\sum_s p(s)\bigg[E\big\{\big(G_t-v_\pi(S_t)\big)^2|S_t=s\big\}\bigg]+MSVE(w) \\
                         \end{align}$$<br/>
      <p>Note that the first term doesn't depend on $w$. Therefore, minimizing MSRE is equivalent to minimize MSVE.
      The solution is an unbiased estimate. <strong>QED</strong></p>

      <h2 id="Solution-2:-Temporal-Difference-Learning-with-Function-Approximation">Solution 2: Temporal Difference Learning with Function Approximation</h2>
      <p>Now the sequence is</p>
      $$\big(S_1,R_2+\gamma\hat{v}(S_2;w)\big),\big(S_2,R_3+\gamma\hat{v}(S_3;w)\big),\dots,\big(S_t,R_{t+1}+\gamma\hat{v}(S_{t+1};w)\big),\dots$$<br/>
      <p>and the update is given by</p>
      <blockquote>$$\begin{align}\Delta w &amp;\approx \alpha\big(R_{t+1}+\gamma\hat{v}(S_{t+1};w)-\hat{v}(s;w)\big)\nabla_w \hat{v}(s;w)\end{align}$$<br/>
      </blockquote>
      <p><strong>Problem</strong>. There is no equivalent objective function from which we can derive this update formula.
      <strong>In this case, the updates are performed online (not exactly supervised learning)</strong>.
      <br><strong>This is a heuristic method with no guarantee about convergence to optimal solution. Furthermore, there are counterexamples showing that this method can diverge. It has been used in many practical cases.</strong></p>
      
      <h2 id="Solution-3:-TD($\lambda$)-Learning-with-Function-Approximation">Solution 3: TD($\lambda$) Learning with Function Approximation</h2>
      <p>Now the sequence is</p>
      $$\big(S_1,G_1^\lambda\big),\big(S_2,G_2^\lambda\big),\dots,\big(S_t,G_t^\lambda\big),\dots,\big(S_t,G_T^\lambda\big)$$<br/>
      <p>and the update is given by</p>
      <blockquote>$$\begin{align}\Delta w &amp;\approx \alpha\big(G_t^\lambda-\hat{v}(s;w)\big)\nabla_w \hat{v}(s;w)\end{align}$$<br/>
      </blockquote>
      <p><strong>Problem</strong>. There is no equivalent objective function from which we can derive this update formula.
      <strong>Nevertheless, it tradeoffs MC and TD</strong></p>

      <h2 id="Solution-4:-Online-TD($\lambda$)-Learning-with-Function-Approximation">Solution 4: Online TD($\lambda$) Learning with Function Approximation</h2>
      <p>Equivalently, we can introduce eligibility traces to make convert TD($\lambda$) to be online.
      <br>Nevertheless, <strong>the definition is different as the eligibility trace is defined on the parameters of the function approximator rather that the states</strong>, namely:</p>
      $$E_0=0$$<br/>
      $$E_t=\gamma\lambda E_{t-1}+\nabla_w \hat{v}(S_t;w)$$<br/>
      <p>and the update rule is</p>
      <blockquote>$$\begin{align}\Delta w &amp;\approx \alpha\big(R_{t+1}+\gamma\hat{v}(S_{t+1};w)-\hat{v}(s;w)\big)E_t\end{align}$$<br/>
      </blockquote>
      <p><strong>Problem</strong>. As for TD($\lambda$), there is no equivalent objective function from which we can derive this update formula.
      <strong>Nevertheless, it tradeoffs MC and TD.</strong></p>

      <h1 id="More-Advanced-Online-Solutions">More Advanced Online Solutions</h1>
      <h2 id="Solution-5:--Residual-Gradient-with-Function-Approximation">Solution 5:  Residual Gradient with Function Approximation</h2>
      <p><strong>Assumption</strong>. Assume that the function approximation <strong>can learn</strong> the true state-value function.
      <br>In order to have an online solution, we can use the <strong>Bellman Error</strong> as an objective. In fact, let's consider the <strong>Mean Squared Bellman Error (MSBE)</strong>, namely:</p>
      $$\begin{align}J(w)&amp;=\frac{1}{2}\sum_s p(s)\bigg[E\big\{\big(R_{t+1}^{S_t}+\gamma\hat{v}(S_{t+1};w)\big)|S_t=s\big\}-\hat{v}(s;w)\bigg]^2\end{align}$$<br/>
      <p>Now by computing the negative gradient, we obtain</p>
      <blockquote>$$\begin{align}\Delta w &amp;= \alpha \sum_s p(s)\bigg[E\big\{\big(R_{t+1}^{S_t}+\gamma\hat{v}(S_{t+1};w)\big)|S_t=s\big\}-\hat{v}(s;w)\bigg]\bigg[\nabla_w \hat{v}(s;w)-\gamma E\big\{\nabla_w\hat{v}(S_{t+1};w)\big)|S_t=s\big\}\bigg] \\
                              &amp;\approx \alpha \big(R_{t+1}^{s}+\gamma\hat{v}(s';w)-\hat{v}(s;w)\big)\big(\nabla_w \hat{v}(s;w)-\gamma \nabla_w\hat{v}(s^{''};w)\big)\end{align}$$<br/>
      </blockquote>
      <p>Note that in order to compute an <strong>unbiased estimate of the gradient</strong>, we need to <strong>sample two successor states, namely $s'$ and $s''$</strong>.
      <br>Note also that the update rule corresponds to <strong>TD update rule + a gradient correction term</strong>.
      <br><strong>Problem</strong>. This solution is not optimal if the main assumption does not hold.</p>

      <h2 id="Solution-6:-TD-with-Gradient-Descent-Methods-and-Linear-Function-Approximation">Solution 6: TD with Gradient Descent Methods and Linear Function Approximation</h2>
      <p><strong>Assumption</strong>. There is no previous assumption. Therefore, this is the most general case.
      <br>Firstly, we have to define a projection operator. See the following picture to understand:</p><br/>
      <p style="text-align:center"><img src="./img/Projection.png" alt="Projection operator" style="width:90%;height:90%;"></p><br/>
      <p>Formally,</p>
      $$\Pi: \Pi v_\pi = \hat{v}_{w'}$$<br/>
      <p>where</p>
      $$w'=\arg\min_w\|\hat{v}_{w}-v_\pi\|_D^2$$<br/>
      <p>and</p>
      $$D=\left[\begin{array}{llll}p(s_1)&amp;&amp;&amp;\\
                                   &amp;p(s_2)&amp;&amp;\\
                                   &amp;&amp;\ddots&amp;\\
                                   &amp;&amp;&amp;p(s_{|S|})\end{array}\right]$$<br/>
      <p>In this case, we are considering <strong>the space of linear function approximators</strong>, $\hat{v}_{w}=\phi w$ where $\phi\in\mathbb{R}^{|S|\times d}$. Now, let's compute $w'$</p>
      $$\|\hat{v}_{w}-v_\pi\|_D^2 = \|\phi w-v_\pi\|_D^2=\big(\phi w-v_\pi\big)^TD\big(\phi w-v_\pi\big)=w^T\phi^TD\phi w-2w^T\phi^TDv_\pi+v_\pi^TDv_\pi$$<br/>
      <p>We can easily compute the minimum with respect to $w'$. In fact, $w'=\big(\phi^TD\phi\big)^{-1}\phi^TDv_\pi$.
      Now, let's use the definition of projection operator</p>
      $$\Pi v_\pi = \hat{v}_{w'}$$<br/>
      $$\Pi v_\pi = \phi w'$$<br/>
      $$\Pi v_\pi = \phi\big(\phi^TD\phi\big)^{-1}\phi^TDv_\pi$$<br/>
      <p>Therefore,</p>
      $$\Pi = \phi\big(\phi^TD\phi\big)^{-1}\phi^TD$$<br/>
      <p>Now, we are ready to define the <strong>Mean Squared Projected Bellman Error (MSPBE)</strong>. Since, we don't have access to $v_\pi$, we can still have an <strong>indication where it is</strong> by exploiting the Bellman operator $T\hat{v}(s;w)=R_{t+1}^s+\gamma E\{\hat{v}(S_{t+1};w)|S_t=s\}$ (which points in the direction where $v_\pi$ is, as we know that the Bellman operator is contraction mapping and the convergence is linear). See the left picture</p><br/>
      <p style="text-align:center"><img src="./img/MSPBE.png" alt="MSPBE" style="width:90%;height:90%;"></p><br/>
      <p>$$\begin{align}J(w)&amp;=\frac{1}{2}\sum_s p(s)\bigg[\hat{v}(s;w)-\Pi_{s:}T\hat{v}(:;w)\bigg]^2\\
                         &amp;=\frac{1}{2}\sum_s p(s)\bigg[\Pi_{s:}\hat{v}(:;w)-\Pi_{s:}T\hat{v}(:;w)\bigg]^2 \quad\text{as the point is projected to itself}\quad \hat{v}(s;w)=\Pi_{s:}\hat{v}(:;w) \\
                         &amp;=\frac{1}{2}\sum_s p(s)\bigg[\Pi_{s:}\big(\hat{v}(:;w)-T\hat{v}(:;w)\big)\bigg]^2 \\
                         &amp;=\frac{1}{2}\big(\hat{v}(:;w)-T\hat{v}(:;w)\big)^T\Pi^T D \Pi \big(\hat{v}(:;w)-T\hat{v}(:;w)\big) \\
                         &amp;=\frac{1}{2}\big(\hat{v}(:;w)-T\hat{v}(:;w)\big)^T\big(\phi\big(\phi^TD\phi\big)^{-1}\phi^TD\big)^T D \big(\phi\big(\phi^TD\phi\big)^{-1}\phi^TD\big) \big(\hat{v}(:;w)-T\hat{v}(:;w)\big) \quad\text{by using}\quad \Pi = \phi\big(\phi^TD\phi\big)^{-1}\phi^TD \\
                         &amp;=\frac{1}{2}\big(\hat{v}(:;w)-T\hat{v}(:;w)\big)^T\big(D^T\phi\big(\phi^TD\phi\big)^{-1}\phi^T\big) D \big(\phi\big(\phi^TD\phi\big)^{-1}\phi^TD\big) \big(\hat{v}(:;w)-T\hat{v}(:;w)\big) \\
                         &amp;=\frac{1}{2}\big(\hat{v}(:;w)-T\hat{v}(:;w)\big)^TD^T\phi\big(\phi^TD\phi\big)^{-1}\big(\phi^T D \phi\big)\big(\phi^TD\phi\big)^{-1}\phi^TD \big(\hat{v}(:;w)-T\hat{v}(:;w)\big) \\
                         &amp;=\frac{1}{2}\big(\hat{v}(:;w)-T\hat{v}(:;w)\big)^TD^T\phi\big(\phi^TD\phi\big)^{-1}\phi^TD \big(\hat{v}(:;w)-T\hat{v}(:;w)\big) \\
                         &amp;=\frac{1}{2}\bigg[\phi^TD\big(\hat{v}(:;w)-T\hat{v}(:;w)\big)\bigg]^T\big(\phi^TD\phi\big)^{-1}\bigg[\phi^TD \big(\hat{v}(:;w)-T\hat{v}(:;w)\big)\bigg] \\
                         &amp;=\frac{1}{2}\big(\phi^TD\delta_w\big)^T\big(\phi^TD\phi\big)^{-1}\big(\phi^TD \delta_w\big) \quad\text{where}\quad \delta_w=\big(\hat{v}(:;w)-T\hat{v}(:;w)\big) \\
                         &amp;=\frac{1}{2}\big(\sum_s p(s)\delta_w(s)\phi_{s:}^T\big)^T\big(\phi^TD\phi\big)^{-1}\big(\sum_s p(s)\delta_w(s)\phi_{s:}^T\big) \\
                         &amp;=\frac{1}{2}\bigg(\sum_s p(s)\big(\hat{v}(s;w)-T\hat{v}(s;w)\big)\phi_{s:}^T\bigg)^T\big(\sum_s p(s)\phi_{s:}^T\phi_{s:}\big)^{-1}\bigg(\sum_s p(s)\big(\hat{v}(s;w)-T\hat{v}(s;w)\big)\phi_{s:}^T\bigg) \\
                         &amp;=\frac{1}{2}E_s\bigg\{\big(\hat{v}(s;w)-T\hat{v}(s;w)\big)\phi_{s:}^T\bigg\}^TE_s\bigg\{\phi_{s:}^T\phi_{s:}\bigg\}^{-1}E_s\bigg\{\big(\hat{v}(s;w)-T\hat{v}(s;w)\big)\phi_{s:}^T\bigg\}
      \end{align}$$<br/>
      <p>Now, we can take the gradient with respect to $w$, but first let's recall that</p>
      $$\nabla_w J(w)=\frac{1}{2}\nabla_w f(w)^T A f(w)=\text{Jac}\big(f(w)\big)^TA^Tf(w)$$<br/>
      <p>where $A=E_s\bigg\{\phi_{s:}^T\phi_{s:}\bigg\}^{-1}$ and $f(w)=\sum_s p(s)\big(\hat{v}(s;w)-T\hat{v}(s;w)\big)\phi_{s:}^T$</p>
      $$\begin{align}
      \text{Jac}\big(f(w)\big)=\sum_s p(s)\text{Jac}\big(\big(\hat{v}(s;w)-T\hat{v}(s;w)\big)\phi_{s:}^T\big)
      &amp;= \sum_s p(s)\left[\begin{array}{lll} \frac{\partial\big(\hat{v}(s;w)-T\hat{v}(s;w)\big)\phi_{s1}}{\partial w_1} &amp; \dots &amp; \frac{\partial \big(\hat{v}(s;w)-T\hat{v}(s;w)\big)\phi_{s1}}{\partial w_d} \\
      \vdots &amp; &amp; \vdots \\
      \frac{\partial\big(\hat{v}(s;w)-T\hat{v}(s;w)\big)\phi_{sd}}{\partial w_1} &amp; \dots &amp; \frac{\partial\big(\hat{v}(s;w)-T\hat{v}(s;w)\big)\phi_{sd}}{\partial w_d}
      \end{array}\right] \\
      &amp;=\sum_s p(s)\phi_{s:}^T \bigg(\nabla_w\big(\hat{v}(s;w)-T\hat{v}(s;w)\big)\bigg)^T \\
      &amp;=\sum_s p(s)\phi_{s:}^T \bigg(\nabla_w\big(\hat{v}(s;w)-R_{t+1}^s-\gamma E\{\hat{v}(S_{t+1};w)|S_t=s\}\big)\bigg)^T \\
      &amp;=\sum_s p(s)\phi_{s:}^T \bigg(\nabla_w\hat{v}(s;w)-\gamma E\{\nabla_w\hat{v}(S_{t+1};w)|S_t=s\}\bigg)^T \\
      &amp;=\sum_s p(s)\phi_{s:}^T \bigg(\nabla_w\phi_{s:}w-\gamma E\{\nabla_w\phi_{S_{t+1}:}w|S_t=s\}\bigg)^T \\
      &amp;=\sum_s p(s)\phi_{s:}^T \bigg(\phi_{s:}^T-\gamma E\{\phi_{S_{t+1}:}^T|S_t=s\}\bigg)^T \\
      &amp;=\sum_s p(s)\phi_{s:}^T \bigg(\phi_{s:}-\gamma E\{\phi_{S_{t+1}:}|S_t=s\}\bigg) \\
      &amp;=\sum_s p(s)\phi_{s:}^T \bigg(\phi_{s:}-\gamma E_{s'|s}\{\phi_{s':}\}\bigg) \quad\text{to simplify notation}\\
      &amp;=E_s\bigg\{\phi_{s:}^T \big(\phi_{s:}-\gamma E_{s'|s}\{\phi_{s':}\}\big)\bigg\}
      \end{align}$$<br/>
      <p>Therefore,</p>
      $$\begin{align}
      \nabla_w J(w) &amp;= E_s\bigg\{\phi_{s:}^T \big(\phi_{s:}-\gamma E_{s'|s}\{\phi_{s':}\}\big)\bigg\}^TE_s\bigg\{\phi_{s:}^T\phi_{s:}\bigg\}^{-1}E_s\bigg\{\big(\hat{v}(s;w)-T\hat{v}(s;w)\big)\phi_{s:}^T\bigg\} \\
                    &amp;= E_s\bigg\{\phi_{s:}^T \big(\phi_{s:}-\gamma E_{s'|s}\{\phi_{s':}\}\big)\bigg\}^TE_s\bigg\{\phi_{s:}^T\phi_{s:}\bigg\}^{-1}E_s\bigg\{\delta_w(s)\phi_{s:}^T\bigg\} \\
                    &amp;= E_s\bigg\{\big(\phi_{s:}-\gamma E_{s'|s}\{\phi_{s':}\}\big)^T \phi_{s:}\bigg\}E_s\bigg\{\phi_{s:}^T\phi_{s:}\bigg\}^{-1}E_s\bigg\{\delta_w(s)\phi_{s:}^T\bigg\}
      \end{align}
      $$<br/>
      <p>From this, we can derive two update rules</p>
      <blockquote><p><strong>Gradient TD (GTD)</strong>.</p>
      $$\Delta w=-\alpha E_s\bigg\{\big(\phi_{s:}-\gamma E_{s'|s}\{\phi_{s':}\}\big)^T \phi_{s:}\bigg\}E_s\bigg\{\phi_{s:}^T\phi_{s:}\bigg\}^{-1}E_s\bigg\{\delta_w(s)\phi_{s:}^T\bigg\}$$<br/>
      <p><strong>TD with Gradient Correction (TDC)</strong>.</p>
      $$\begin{align}\Delta w &amp;= -\alpha E_s\bigg\{\big(\phi_{s:}-\gamma E_{s'|s}\{\phi_{s':}\}\big)^T \phi_{s:}\bigg\}E_s\bigg\{\phi_{s:}^T\phi_{s:}\bigg\}^{-1}E_s\bigg\{\delta_w(s)\phi_{s:}^T\bigg\} \\
                              &amp;= \alpha E_s\bigg\{\gamma E_{s'|s}\{\phi_{s':}\}^T\phi_{s:}-\phi_{s:}^T\phi_{s:}\bigg\}E_s\bigg\{\phi_{s:}^T\phi_{s:}\bigg\}^{-1}E_s\bigg\{\delta_w(s)\phi_{s:}^T\bigg\} \\
                              &amp;= \alpha E_s\bigg\{\gamma E_{s'|s}\{\phi_{s':}\}^T\phi_{s:}\bigg\}E_s\bigg\{\phi_{s:}^T\phi_{s:}\bigg\}^{-1}E_s\bigg\{\delta_w(s)\phi_{s:}^T\bigg\}-\alpha E_s\bigg\{\delta_w(s)\phi_{s:}^T\bigg\} \\\end{align}$$<br/>
      </blockquote>
      <p>Note that the term $E_s\bigg\{\phi_{s:}^T\phi_{s:}\bigg\}^{-1}E_s\bigg\{\delta_w(s)\phi_{s:}^T\bigg\}$ requires two independent samples for $s$, but there is a smart way to use a <strong>single</strong> sample for $s$. In fact, let's set</p>
      $$\theta=E_s\bigg\{\phi_{s:}^T\phi_{s:}\bigg\}^{-1}E_s\bigg\{\delta_w(s)\phi_{s:}^T\bigg\}$$<br/>
      <p>We can obtain $\theta$ by minimizing the following objective</p>
      $$J'(\theta)=E_s\bigg\{\big(\phi_{s:}\theta - \delta_w(s)\big)^2\bigg\}$$<br/>
      <p>In fact, $\nabla_\theta J'(\theta)= 2E_s\big\{\phi_{s:}^T\phi_{s:}\big\}\theta-2E_s\big\{\delta_w(s)\phi_{s:}^T\big\}=0$. Now, let's derive an update rule for $\theta$, namely:</p>
      $$\begin{align}\Delta\theta &amp;= -\frac{\beta}{2}E_s\bigg\{2\big(\phi_{s:}\theta - \delta_w(s)\big)\phi_{s:}^T\bigg\} \\
                                  &amp;= -\beta E_s\bigg\{\big(\phi_{s:}\theta - \delta_w(s)\big)\phi_{s:}^T\bigg\}\end{align}$$<br/>
      <blockquote><p><strong>Gradient TD (GTD)</strong>.</p>
      $$\Delta w \approx -\alpha \big(\phi_{s:}-\gamma\phi_{s':}\big)^T \phi_{s:}\theta$$<br/>
      $$\Delta\theta \approx -\beta \big(\phi_{s:}\theta - \delta_w(s)\big)\phi_{s:}^T$$<br/>
      $$\delta_w(s)\approx\hat{v}(s;w)-R^s-\gamma\hat{v}(s';w)$$<br/>
      <p><strong>TD with Gradient Correction (TDC)</strong>.</p>
      $$\Delta w \approx \alpha\gamma\phi_{s':}^T\phi_{s:}\theta-\alpha\delta_w(s)\phi_{s:}^T$$<br/>
      $$\Delta\theta \approx -\beta \big(\phi_{s:}\theta - \delta_w(s)\big)\phi_{s:}^T$$<br/>
      $$\delta_w(s)\approx\hat{v}(s;w)-R^s-\gamma\hat{v}(s';w)$$<br/>
      </blockquote>
      <p>Proof of convergence of GTD is given in <a href="https://epubs.siam.org/doi/10.1137/S0363012997331639">Borkar 2000</a>
      Proof of convergence of TDC is given in <a href="https://www.sciencedirect.com/science/article/pii/S0167691197900153">Borkar 1997</a></p>
      
      <h2 id="Solution-7:-TD-with-Gradient-Descent-Methods-and-Non-linear-Function-Approximation">Solution 7: TD with Gradient Descent Methods and Non-linear Function Approximation</h2>
      <p>The idea is to consider <strong>the tangent space of the parameterized space</strong>, by linearizing it and then apply the strategy used in previous solution.
      For more details, check my notes and the paper of <a href="https://papers.nips.cc/paper/3809-convergent-temporal-difference-learning-with-arbitrary-smooth-function-approximation.pdf">Maei 2009</a>.</p>
      <h2 id="Summary">Summary</h2>
      <p>We have seen several solutions with function approximation:</p>
      <ul>
      <li><strong>Episodic MC</strong>, whose objective is <strong>MSRE</strong></li>
      <li><strong>Online TD</strong></li>
      <li><strong>Episodic TD($\lambda$)</strong></li>
      <li><strong>Online TD($\lambda$)</strong></li>
      <li>Advanced: <strong>Residual Gradients</strong>, whose objective is <strong>MSBE</strong></li>
      <li>Advanced: <strong>Gradient TD</strong> (2 algorithms for both linear/nonlinear function approx.), whose objective is <strong>MSPBE</strong></li>
      </ul>
      <blockquote><p><strong>Premise</strong>. In <strong>prediction</strong> problems we can still have <strong>on-policy</strong> and <strong>off-policy learning</strong>.
      <br>In fact, if the MDP is <strong>ergodic</strong> then we can accumulate samples in form of tuples and perform supervised learning.
      <br>The tuple consists of $&lt;S_t,A_t,R_{t+1},S_{t+1},(A_{t+1})&gt;$, where $A_t$ is the action taken by the <strong>behaviour policy</strong> $b$ (the one acting in the environment) and $A_{t+1}$ is the action taken by the <strong>target policy</strong> $\pi$ (the one imagining to act). In on-policy learning $b=\pi$, while in off-policy learning $b\neq\pi$.
      <br>This is important, because <strong>the behaviour policy induces a prior over states</strong>, namely <strong>a stationary distribution</strong> $p(s)$ for all $s$, which is used as weigthing factor in the objectives we have defined so far. Analytically, it can be computed by considering $p(s'|s)=\sum_a b(a|s)p(s'|a,s)$, defining matrix $P$ with elements given by $p(s'|s)$ and solving equation $p=P^Tp$. In reality, we don't need to compute this distribution, but we have to be aware of that especially for off-policy learning, because **this weights each state in a different way from the distribution induced by the target policy, thus leading to unexpected algorithm behaviours. (See following considerations).</p>
      </blockquote>
      <p>To analyze the properties of the algorithms, <strong>convergence</strong> and <strong>convergence to optimal solutions</strong> we have to distinguish two cases.
      <br><strong>NOT AGNOSTIC (True value function is in the parameterized value space)</strong>
      <br>In this case, all algorithms discussed converge to optimal solutions
      <br><strong>AGNOSTIC (True value function is NOT in the parameterized value space)</strong>
      <br>In this case, <strong>Online TD, Episodic/Online TD($\lambda$) can diverge</strong>, because they do not solve any objective.
      In on-policy learning, divergence can be observed <strong>only</strong> with non-linear function approximation, while in off-policy learning, divergence can be observed <strong>even</strong> with linear function approximation.
      <br>All other algorithms <strong>converge</strong>.
      <br>But, in this scenario it is better to consider <strong>only two objectives</strong>, namely MSRE and MSPBE. Furthermore, it is important to consider the proper <strong>weighting</strong> in <strong>off-policy learning</strong>, in order to ensure to <strong>convergence to good solutions</strong>.
      <br><strong>For practical agnostic problems, it becomes difficult to ensure convergence to good solutions</strong>.</p>
      
      <h1 id="Incremental-Methods:-Control">Incremental Methods: Control</h1>
      <p><strong>The typical strategy is to alternate a step of policy evaluation (from previous section) with policy improvement.</strong></p>
      <p>In general, proving convergence in control is even more difficult as the behaviour policy changes over time (most results show convergence for linear function approximation).
      Nevertheless, this problem can be addressed directly by the following fundamental question (which is the SOTA of incremental methods in RL).</p>
      
      <h1 id="Batch-Methods">Batch Methods</h1>
      <p>The main idea to learn a function approximator is to use <strong>supervised learning</strong>. But, <strong>samples are not iid distributed</strong> in this context. A solution to restore the <strong>iid assumption</strong> consists of <strong>storing all samples seen so far (including the current one) and then sample randomly a batch of them to update the function approximator</strong>. This heuristic strategy is called <strong>experience replay</strong>.</p>
      <blockquote><p>For <strong>linear function approximator</strong>. Experience replay allows to formulate the problem of prediction as a least squares problem. namely
      $$J(w)=\sum_{\text{batch}}\big(\text{target}-\hat{q}_\pi(a,s;w)\big)^2$$
      and the target can be $G_t$ for Least Squares Monte Carlo (LSMC), $R_{t+1}+\gamma \hat{q}_\pi(a',s';w)$ for Least Squares TD (LSTD) and $G_t^\lambda$ for Least Squares TD($\lambda$) (LSTD($\lambda$)).  <strong>The solution of the least squares problem can be computed efficiently in a single step</strong> (i.e. instead of computing an inverse matrix at each iteration, you can exploit the Sherman-Morrison identity to perform the inversion only once at the beginnning of training).</p>
      <p>For <strong>nonlinear function approximator</strong>. Experience replay stabilizes training using TD learning (Q-learning). Another heuristic helpful for stabilization consists of using an old parameter vector in the TD target, namely $R_{t+1}+\gamma \hat{q}_\pi(a',s';w_{\text{old}})$. These two ideas are the main contributions of the important <a href="https://www.nature.com/articles/nature14236">Deep Q learning</a> paper.</p>
      </blockquote>
           
    </div><!-- /.article-wrap -->

    <div id="disqus_thread" style="width:140%;text-align:justify;margin-top:3cm;margin-bottom:2cm;"></div>
    <script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://ema87.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
    </script>
            
  </article>
</div><!-- /#index -->
                    

<div class="footer-wrap">
  <footer>
<span>&copy; 2020 Emanuele Sansone.

  </span></footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="https://emsansone.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="https://emsansone.github.io/assets/js/scripts.min.js"></script>

</body></html>
