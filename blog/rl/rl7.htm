<!DOCTYPE html>

<head>
<!-- saved from url=(0028) https://emsansone.github.io/ -->
<!-- <html class=" js no-touch rgba hsla textshadow opacity svg" lang="en"><!--<![endif]-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Emanuele Sansone</title>
<meta name="description" content="Emanuele Sansone">
<meta name="keywords" content="Jekyll, theme, responsive, blog, template">


<!-- Twitter Cards -->
<meta name="twitter:title" content="Emanuele Sansone">
<meta name="twitter:description" content="Emanuele Sansone">



<!--<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://emsansone.github.io/img/Background3.jpg">-->

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Emanuele Sansone">
<meta property="og:description" content="Emanuele Sansone">
<meta property="og:url" content="https://emsansone.github.io/blog/">
<meta property="og:site_name" content="Emanuele Sansone">





<link rel="canonical" href="https://emsansone.github.io/blog/">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="https://emsansone.github.io/assets/css/main.css">
    
<meta http-equiv="cleartype" content="on">
        
<!-- Modernizr -->
<script src="https://emsansone.github.io/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>
        
<link href='//fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700%7CPT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>


<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="https://emsansone.github.io/img/logo.png">
<!-- 32x32 -->
<link rel="shortcut icon" href="https://emsansone.github.io/img/logo-32x32.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="https://emsansone.github.io/img/logo-57x57.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://emsansone.github.io/img/logo-72x72.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://emsansone.github.io/img/logo-114x114.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://emsansone.github.io/img/logo-144x144.png">

<style>
  table, th, td {
    border: 1px solid grey;
    border-collapse: collapse;
  }
</style>

</head>

<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    // Center justify equations in code and markdown cells. Elsewhere
    // we use CSS to left justify single line equations in code cells.
    displayAlign: 'center',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}},
        linebreaks: { automatic: true }
    }
});
</script>

<body class="page">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="https://emsansone.github.io/">Website</a>
	</div><!-- /.site-name -->
    <div class="top-navigation">
        <nav role="navigation" id="site-nav" class="nav">
		    <ul>
              <li><a href="https://emsansone.github.io/blog/rl/rl0.htm">Index</a></li>	        					    
					    <li><a href="https://emsansone.github.io/blog/rl/rl6.htm" >Previous Lecture</a></li>
					    <li><a href="https://emsansone.github.io/blog/rl/rl8.htm" >Next Lecture</a></li>

		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->


<!--<div class="image-wrap">
    <img src="https://emsansone.github.io/img/Background3.jpg" alt="Emanuele Sansone feature image">
</div>-->

<div id="main" role="main">
  <div class="article-author-side">
    <img src="https://emsansone.github.io/img/Me.jpg" class="bio-photo" alt="Emanuele Sansone bio photo">

    <h2>Emanuele Sansone</h2>
    <p>PhD in machine learning and artificial intelligence.</p>
    <a href="mailto:e.sansone@hotmail.it" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/email.png" height=14px width=14px> Email</a>

    <a href="https://linkedin.com/in/emanuele-sansone-97329475" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/linkedin.png" height=14px width=14px> LinkedIn</a>

    <a href="https://twitter.com/skiera87" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/twitter.png" height=14px width=14px> Twitter</a>

    <!-- <a href="https://github.com/emsansone" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/github.png" height=14px width=14px> Github</a> -->
  </div>

  <article class="page">

    <div class="article-wrap" style="width:140%;text-align:justify;">
    
      <h1 id="Lecture-7:-Policy-Gradient">Policy Gradient</h1>
      <p>Here is the summary of the lecture.</p><br/>
      <p style="text-align: center;"><img src="./img/Gradient.png" alt="Policy Gradient"></p><br/>
      <p>Policy-based algorithms use a <strong>parameterized policy</strong>, namely $\pi(a|s,\theta)$, while value-based algorithms use an $\epsilon$-greedy policy.
      This is a summary comparing the two classes of algorithms.</p>
      <table>
      <thead><tr>
      <th style="text-align:center">Algorithms</th>
      <th style="text-align:left">Advantages</th>
      <th style="text-align:left">Disadvantages</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td style="text-align:center">Value-based</td>
      <td style="text-align:left">Opposite to Policy-based</td>
      <td style="text-align:left">Opposite to Policy-based</td>
      </tr>
      <tr>
      <td style="text-align:center">Policy-based</td>
      <td style="text-align:left">1. Faster convergence (small change in $\theta$ translates into smooth change in policy) <br> 2. No max operator (better for high-dimensional action space, continuous space) <br> 3. Stochastic policy (in POMDPs or in contexts where the feature representation is not good enough stochastic policy may be optimal. Recall that for MDPs there is always an optimal policy)</td>
      <td style="text-align:left">1. Convergence to local optimum <br> 2. High variance</td>
      </tr>
      </tbody>
      </table>

      <h1 id="Objectives-for-policy-based-algorithms">Objectives for policy-based algorithms</h1>
      <p>There are two objectives (but there can be more, see Ideas at the end):</p>
      <blockquote><p><strong>Objective for episodic case</strong>. Given an initial state $s_0$ we define</p>
        $$J(\theta)=v_{\pi_\theta}(\theta)$$<br/>
      <p><strong>Objective for continuing case</strong>. Given an initial state $s_0$, we compute the average reward per time-step, namely</p>
      $$\begin{align}
          J(\theta) &amp;= \lim_{h\rightarrow\infty}\frac{1}{h}\sum_{t=1}^h E\{r_t|\pi\} \\
                    &amp;= \sum_s\mu_\theta(s)\sum_a\pi(a|s,\theta)\sum_{s'}P_{ss'}^aR_{s'}^a \quad\text{by ergodicity}
      \end{align}$$<br/>
      <p></p>Note that $J(\theta)$ is independent of the initial state. By definition of average reward, we redefine the state-value and the action-value functions in the following way:</p>
      $$\begin{align}
          v_{\pi_\theta}(s) &amp;= E\{G_t|S_t=s\} \\
          q_{\pi_\theta}(a,s) &amp;= E\{G_t|S_t=s,A_t=a\} \\
          G_t &amp;= R_{t+1} - J(\theta) + \gamma\big(R_{t+2} - J(\theta)\big) + \dots
      \end{align}$$<br/>
      </blockquote>
      <p>Now that we have an objective, we can optimize it!
      <br><strong>Question</strong>. How can we optimize it?
      <br><strong>Answers</strong>. There are several classes of optimizers, namely:</p>
      <ol>
      <li><strong>Gradient-free methods</strong>, like finite difference policy gradient</li>
      <li><strong>First-order methods</strong>, which are based on an unbiased estimate of the gradient<br/><br/>
        <span class="math display">\[\begin{align}
            \theta_{t+1} = \arg &amp; \max_\theta J(\theta) \\
                                &amp; \text{s.t. } \|\theta-\theta_t\|\leq\epsilon_t
        \end{align}\]</span><br/>
      </li>
      <li><strong>Natural gradient methods</strong>, which are based on an unbiased estimate of the gradient under different assumptions from first-order methods<br/><br/>
        <span class="math display">\[\begin{align}
            \theta_{t+1} = \arg &amp; \max_\theta J(\theta) \\
                                &amp; \text{s.t. } KL\big\{\pi_{\theta_t}\|\pi_\theta\big\}\leq\epsilon_t
        \end{align}\]</span><br/>
      </li>
      <li><strong>Second-order methods</strong>, which takes into account the second order information</li>
      </ol>

      <h1 id="First-Order-Methods">First-Order Methods</h1>
      <p>The <strong>Policy gradient theorem</strong> provides a way to estimate the gradient of $J(\theta)$. However, policy gradient algorithms have high variance. Later, we will consider a trick to deal with that, viz. using a baseline.</p>
      
      <h2 id="Policy-Gradient-Theorem">Policy Gradient Theorem</h2>
      <blockquote><p><strong>Theorem</strong>. Given an initial state $s_0$ and an objective function $J(\theta)$ (no matter if we are in the episodic or continuing case).</p>
      $$\nabla_\theta J(\theta) \propto E\big\{q_{\pi_\theta}(A,S)\nabla_\theta\ln\big(\pi(A|S,\theta)\big)\big\}$$<br/>
      </blockquote>
      <p><strong>Proof</strong>. Firstly, we prove that $\nabla_\theta J(\theta) \propto \sum_s\mu_\theta(s)\sum_aq_{\pi_\theta}(a,s)\nabla_\theta\pi(a|s,\theta)$ for both the episodic and the continuing case. 
        <br>Secondly we show that,
      $\sum_s\mu_\theta(s)\sum_aq_{\pi_\theta}(a,s)\nabla_\theta\pi(a|s,\theta)=E\big\{q_{\pi_\theta}(A,S)\nabla_\theta\ln\big(\pi(A|S,\theta)\big)\big\}$.
      <br><strong>[Episodic case]</strong>. Let's consider the episodic case,</p>
      <span class="math display">\[\begin{align}
        \nabla_\theta v_{\pi_\theta}(s) &amp;= \nabla_\theta\sum_a\pi(a|s,\theta)q_{\pi_\theta}(a,s) \\
                                        &amp;= \sum_a\big\{\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s)+\pi(a|s,\theta)\nabla_\theta q_{\pi_\theta}(a,s)\big\} \\
                                        &amp;= \sum_a\big\{\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s)+\pi(a|s,\theta)\nabla_\theta \big[R_s^a+\sum_{s&#39;}P_{ss&#39;}^av_{\pi_\theta}(s&#39;)\big]\big\} \\
                                        &amp;= \sum_a\big\{\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s)+\pi(a|s,\theta)\sum_{s&#39;}P_{ss&#39;}^a\nabla_\theta v_{\pi_\theta}(s&#39;)\big\} \\
                                        &amp;= \sum_a\bigg\{\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s)+\pi(a|s,\theta) \\
                                        &amp;\quad\quad\sum_{s&#39;}P_{ss&#39;}^a\bigg[\sum_{a&#39;}\big\{\nabla_\theta\pi(a&#39;|s&#39;,\theta)q_{\pi_\theta}(a&#39;,s&#39;)+\pi(a&#39;|s&#39;,\theta)\sum_{s^{&#39;&#39;}}P_{s&#39;s^{&#39;&#39;}}^{a&#39;}\nabla_\theta v_{\pi_\theta}(s^{&#39;&#39;})\big\}\bigg]\bigg\} \\
                                        &amp;= \sum_a\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s)+\sum_a\bigg\{\pi(a|s,\theta)\sum_{s&#39;}P_{ss&#39;}^a\sum_{a&#39;}\nabla_\theta\pi(a&#39;|s&#39;,\theta)q_{\pi_\theta}(a&#39;,s&#39;)\bigg\}+ \\
                                        &amp;\quad\quad\sum_a\bigg\{\pi(a|s,\theta)\sum_{s&#39;}P_{ss&#39;}^a\sum_{a&#39;}\pi(a&#39;|s&#39;,\theta)\sum_{s^{&#39;&#39;}}P_{s&#39;s^{&#39;&#39;}}^{a&#39;}\nabla_\theta v_{\pi_\theta}(s^{&#39;&#39;})\bigg\} \\
                                        &amp;= \sum_a\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s)+P(s\rightarrow s&#39;)\sum_{a&#39;}\nabla_\theta\pi(a&#39;|s&#39;,\theta)q_{\pi_\theta}(a&#39;,s&#39;)+ \\
                                        &amp;\quad\quad\sum_a\bigg\{\pi(a|s,\theta)\sum_{s&#39;}P_{ss&#39;}^a\sum_{a&#39;}\pi(a&#39;|s&#39;,\theta)\sum_{s^{&#39;&#39;}}P_{s&#39;s^{&#39;&#39;}}^{a&#39;}\nabla_\theta v_{\pi_\theta}(s^{&#39;&#39;})\bigg\} \\
                                        &amp;= \dots \quad \text{By expliciting the last term in recursive way} \\
                                        &amp;= \sum_{x\in S}\sum_{k=0}^\infty P(s\rightarrow x,k,\pi)\sum_a\nabla_\theta\pi(a|x,\theta)q_{\pi_\theta}(a,x) \\
                                        &amp;\quad\quad\text{where }P(s\rightarrow x,k,\pi)\text{ is the probability to go from s to x in k steps} \\
                                        &amp;= \sum_{x\in S}\eta(x)\sum_a\nabla_\theta\pi(a|x,\theta)q_{\pi_\theta}(a,x) \quad\text{where }\eta(x)=\sum_{k=0}^\infty P(s\rightarrow x,k,\pi) \\
                                        &amp;= \sum_{s\in S}\eta(s)\sum_a\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s) \\
                                        &amp;= \frac{\sum_{s&#39;\in S}\eta(s&#39;)}{\sum_{s&#39;\in S}\eta(s&#39;)}\sum_{s\in S}\eta(s)\sum_a\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s) \\
                                        &amp;= \sum_{s&#39;\in S}\eta(s&#39;)\sum_{s\in S}\mu_\theta(s)\sum_a\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s) \quad\text{where }\mu_\theta(s)=\frac{\eta(s)}{\sum_{s&#39;\in S}\eta(s&#39;)} \\
                                        &amp;\propto \sum_{s\in S}\mu_\theta(s)\sum_a\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s)\\
    \end{align}\]</span><br/>
      <p><strong>[Continuing case]</strong>. Let's consider the continuing case (<strong>note that in this case we cannot directly compute the gradient of $J(\theta)$ as the term $\mu_\theta(s)$ is unknwown</strong>),</p>
      $$\begin{align}
          \nabla_\theta v_{\pi_\theta}(s) &amp;= \nabla_\theta\sum_a\pi(a|s,\theta)q_{\pi_\theta}(a,s) \\
                                          &amp;= \sum_a\big\{\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s)+\pi(a|s,\theta)\nabla_\theta q_{\pi_\theta}(a,s)\big\} \\
                                          &amp;= \sum_a\big\{\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s)+\pi(a|s,\theta)\nabla_\theta \big[R_s^a-J(\theta)+\sum_{s'}P_{ss'}^av_{\pi_\theta}(s')\big]\big\} \\
                                          &amp;= \sum_a\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s)-\nabla_\theta J(\theta)+\sum_a\pi(a|s,\theta)\sum_{s'}P_{ss'}^a\nabla_\theta v_{\pi_\theta}(s') \\
      \end{align}$$<br/>
      <p>Now, we can explicit the gradient of $J(\theta)$, namely:</p>
      $$\nabla_\theta J(\theta)=\sum_a\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s)-\nabla_\theta v_{\pi_\theta}(s)+\sum_a\pi(a|s,\theta)\sum_{s'}P_{ss'}^a\nabla_\theta v_{\pi_\theta}(s')$$<br/>
      <p>In other words,</p>
      $$\begin{align}
          \nabla_\theta J(\theta) &amp;= \sum_a\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s)-\nabla_\theta v_{\pi_\theta}(s)+\sum_a\pi(a|s,\theta)\sum_{s'}P_{ss'}^a\nabla_\theta v_{\pi_\theta}(s') \\
                                  &amp;= \sum_a\bigg[\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s)+\pi(a|s,\theta)\sum_{s'}P_{ss'}^a\nabla_\theta v_{\pi_\theta}(s')\bigg]-\nabla_\theta v_{\pi_\theta}(s) \\
      \end{align}$$<br/>
      <p>Now we can sum both terms by $\sum_s\mu_\theta(s)$ (keep in mind that $\sum_s\mu_\theta(s)\nabla_\theta J(\theta)=\nabla_\theta J(\theta)$)</p>
      $$\begin{align}
          \nabla_\theta J(\theta) &amp;= \sum_s\mu_\theta(s)\sum_a\bigg[\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s)+\pi(a|s,\theta)\sum_{s'}P_{ss'}^a\nabla_\theta v_{\pi_\theta}(s')\bigg]-\sum_s\mu_\theta(s)\nabla_\theta v_{\pi_\theta}(s) \\
                                  &amp;= \sum_s\mu_\theta(s)\sum_a\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s)+\sum_s\mu_\theta(s)\sum_a\pi(a|s,\theta)\sum_{s'}P_{ss'}^a\nabla_\theta v_{\pi_\theta}(s')-\sum_s\mu_\theta(s)\nabla_\theta v_{\pi_\theta}(s) \\
                                  &amp;= \sum_s\mu_\theta(s)\sum_a\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s)+\sum_{s'}\sum_s\mu_\theta(s)\sum_a\pi(a|s,\theta)P_{ss'}^a\nabla_\theta v_{\pi_\theta}(s')-\sum_s\mu_\theta(s)\nabla_\theta v_{\pi_\theta}(s) \\
                                  &amp;= \sum_s\mu_\theta(s)\sum_a\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s)+\sum_{s'}\sum_s\mu_\theta(s)P(s\rightarrow s')\nabla_\theta v_{\pi_\theta}(s')-\sum_s\mu_\theta(s)\nabla_\theta v_{\pi_\theta}(s) \\
                                  &amp;\quad\quad\text{where }P(s\rightarrow s')=\sum_a\pi(a|s,\theta)P_{ss'}^a \\
                                  &amp;= \sum_s\mu_\theta(s)\sum_a\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s)+\sum_{s'}\mu_\theta(s')\nabla_\theta v_{\pi_\theta}(s')-\sum_s\mu_\theta(s)\nabla_\theta v_{\pi_\theta}(s) \\
                                  &amp;\quad\quad\text{recall that }\sum_s\mu_\theta(s)P(s\rightarrow s')=\mu_\theta(s') \\
                                  &amp;= \sum_s\mu_\theta(s)\sum_a\nabla_\theta\pi(a|s,\theta)q_{\pi_\theta}(a,s)
      \end{align}$$<br/>
      <p><strong>[Last step]</strong>.</p>
      $$\begin{align}
          \nabla_\theta J(\theta) &amp;\propto \sum_s\mu_\theta(s)\sum_aq_{\pi_\theta}(a,s)\nabla_\theta\pi(a|s,\theta) \\
                                  &amp;= E\big\{\sum_aq_{\pi_\theta}(a,S)\nabla_\theta\pi(a|S,\theta)\big\} \\
                                  &amp;\quad\quad\text{we have defined random variable S and compute its expectation conditioned on initial state }s_0 \\
                                  &amp;= E\bigg\{\sum_a\pi(a|S,\theta)q_{\pi_\theta}(a,S)\frac{\nabla_\theta\pi(a|S,\theta)}{\pi(a|S,\theta)}\bigg\} \\
                                  &amp;= E\bigg\{q_{\pi\theta}(A,S)\frac{\nabla_\theta\pi(A|S,\theta)}{\pi(A|S,\theta)}\bigg\} \quad\text{similarly, we have defined random variable A}\\
                                  &amp;= E\big\{q_{\pi_\theta}(A,S)\nabla_\theta\ln\pi(A|S,\theta)\big\}\\
      \end{align}$$
      <strong>QED</strong><br/>

      <h2 id="Estimate-1:-Monte-Carlo-Policy-Gradient-(REINFORCE)">Estimate 1: Monte Carlo Policy Gradient (REINFORCE)</h2>
      $$\begin{align}
          \nabla_\theta J(\theta) &amp;\propto E\big\{q_{\pi\theta}(A,S)\nabla_\theta\ln\pi(A|S,\theta)\big\}\\
                                  &amp;= E\big\{E\{G_t|A'=A,S'=S\}\nabla_\theta\ln\pi(A|S,\theta)\big\}\\
                                  &amp;= E\big\{G\nabla_\theta\ln\pi(A|S,\theta)\big\} \quad\text{as }E\{G|A'=A,S'=S\}=P(A'=A,S'=S)G=G \\
                                  &amp;\approx g_t\nabla_\theta\ln\pi(a|s,\theta)
      \end{align}$$<br/>
      <p>Note that this algorithm works only in the episodic case, since we need the return, which is available only at the end. The return is a summation of all rewards and therefore it introduces a lot of variance.</p>
      
      <h2 id="Estimate-2:-Actor-Critic-Policy-Gradient">Estimate 2: Actor-Critic Policy Gradient</h2>
      <p>Another strategy (which has less variance) can be obtained by using an action-value function approximator</p>
      $$\begin{align}
          \nabla_\theta J(\theta) &amp;\propto E\big\{q_{\pi\theta}(A,S)\nabla_\theta\ln\pi(A|S,\theta)\big\}\\
                                  &amp;\approx Q_w(a,s)\nabla_\theta\ln\pi(a|s,\theta)
      \end{align}$$<br/>
      <p>Therefore, we can leverage all knowledge we have to perform policy evaluation to learn $Q_w(a,s)$ and then learn the policy function approximator.</p>
      
      <h2 id="Policy-Gradient-Theorem-with-Baseline">Policy Gradient Theorem with Baseline</h2>
      <blockquote><p><strong>Theorem</strong>. Given an initial state $s_0$ and an objective function $J(\theta)$ (no matter if we are in the episodic or continuing case).
      $$\nabla_\theta J(\theta) \propto E\big\{\big(q_{\pi_\theta}(A,S)-v_{\pi_\theta}(S)\big)\nabla_\theta\ln\big(\pi(A|S,\theta)\big)\big\}$$</p>
      </blockquote>
      <p><strong>Proof</strong>. We can extend the normal policy gradient theorem, by introducing a baseline function $B(s)$, namely</p>
      $$\begin{align}
          E\big\{B(S)\nabla_\theta\ln\pi_\theta(A|S)\big\} &amp;= \sum_s\mu_\theta(s)\sum_a\pi(a|s,\theta)B(S)\nabla_\theta\ln\pi_\theta(A|S) \\
          &amp;= \sum_s\mu_\theta(s)B(S)\sum_a\pi(a|s,\theta)\nabla_\theta\ln\pi_\theta(A|S) \\
          &amp;= \sum_s\mu_\theta(s)B(S)\sum_a\nabla_\theta\pi_\theta(A|S) \\
          &amp;= \sum_s\mu_\theta(s)B(S)\nabla_\theta\sum_a\pi_\theta(A|S) \\
          &amp;= \sum_s\mu_\theta(s)B(S)\nabla_\theta 1 \\
          &amp;= 0
      \end{align}$$<br/>
      <p>This result is valid for any baseline function. Let's choose $B(s)=v_{\pi_\theta}(s)$. The result of the theorem follows in straightforward way. <strong>QED</strong><br/>
      <br>Note that $q_{\pi_\theta}(a,s)-v_{\pi_\theta}(s)$ is usually called the <strong>advantage function</strong> (as it measures what is the contribution of action a with respect to the expected value).</p>
      
      <h2 id="Estimate-1:--Monte-Carlo-Policy-Gradient-with-Baseline">Estimate 1:  Monte Carlo Policy Gradient with Baseline</h2>
      $$\begin{align}
          \nabla_\theta J(\theta) &amp;\propto E\big\{\big(q_{\pi_\theta}(A,S)-v_{\pi_\theta}(S)\big)\nabla_\theta\ln\big(\pi(A|S,\theta)\big)\big\} \\
                                  &amp;\approx \big(g-V_v(s)\big)\nabla_\theta\ln\big(\pi(a|s,\theta)\big)
      \end{align}$$<br/>
      <p>Variance can be still reduced. Furthemore, we need to wait until the end of episode.</p>

      <h2 id="Estimate-2:-Advantage-Actor-Critic-Policy-Gradient">Estimate 2: Advantage Actor-Critic Policy Gradient</h2>
      $$\begin{align}
          \nabla_\theta J(\theta) &amp;\propto E\big\{\big(q_{\pi_\theta}(A,S)-v_{\pi_\theta}(S)\big)\nabla_\theta\ln\big(\pi(A|S,\theta)\big)\big\} \\
                                  &amp;\approx \big(Q_w(a,s)-V_v(s)\big)\nabla_\theta\ln\big(\pi(a|s,\theta)\big)
      \end{align}$$<br/>
      <p>The problem of this approach is that we need three function approximators (lots of parameters). See next estimate for using only two.
      Alternatively, check ideas.</p>

      <h2 id="Estimate-3:-TD-Actor-Critic-Policy-Gradient">Estimate 3: TD Actor-Critic Policy Gradient</h2>
      $$\begin{align}
          \nabla_\theta J(\theta) &amp;\propto E\big\{\big(q_{\pi_\theta}(A,S)-v_{\pi_\theta}(S)\big)\nabla_\theta\ln\big(\pi(A|S,\theta)\big)\big\} \\
          &amp;= E\big\{\big(E\{G|\tilde{A}=A,\tilde{S}=S\}-v_{\pi_\theta}(S)\big)\nabla_\theta\ln\big(\pi(A|S,\theta)\big)\big\} \\
          &amp;= E\big\{\big(E\{R+\gamma v_{\pi_\theta}(S')|\tilde{A}=A,\tilde{S}=S\}-v_{\pi_\theta}(S)\big)\nabla_\theta\ln\big(\pi(A|S,\theta)\big)\big\} \\
          &amp;= E\big\{\big(E\{R+\gamma v_{\pi_\theta}(S')-v_{\pi_\theta}(\tilde{S})|\tilde{A}=A,\tilde{S}=S\}\big)\nabla_\theta\ln\big(\pi(A|S,\theta)\big)\big\} \\
          &amp;= E\big\{\big(R+\gamma v_{\pi_\theta}(S')-v_{\pi_\theta}(S)\big)\nabla_\theta\ln\big(\pi(A|S,\theta)\big)\big\} \\
          &amp;\approx \big(R+\gamma V_v(s')-V_v(s)\big)\nabla_\theta\ln\big(\pi(a|s,\theta)\big) \\
      \end{align}$$<br/>

      <h2 id="Estimate-4:-TD($\lambda$)-Actor-Critic-Policy-Gradient-(Forward-View)">Estimate 4: TD($\lambda$) Actor-Critic Policy Gradient (Forward View)</h2>
      $$\begin{align}
          \nabla_\theta J(\theta) &amp;\propto E\big\{\big(q_{\pi_\theta}(A,S)-v_{\pi_\theta}(S)\big)\nabla_\theta\ln\big(\pi(A|S,\theta)\big)\big\} \\
          &amp;\approx \big(g^\lambda-V_v(s)\big)\nabla_\theta\ln\big(\pi(a|s,\theta)\big) \\
      \end{align}$$<br/>
      <p>In this case, we need to wait until the end of the episode.</p>

      <h2 id="Estimate-5:-TD($\lambda$)-Actor-Critic-Policy-Gradient-(Forward-View)">Estimate 5: TD($\lambda$) Actor-Critic Policy Gradient (Forward View)</h2>
      $$\begin{align}
          \nabla_\theta J(\theta) &amp;\propto E\big\{\big(q_{\pi_\theta}(A,S)-v_{\pi_\theta}(S)\big)\nabla_\theta\ln\big(\pi(A|S,\theta)\big)\big\} \\
          &amp;\approx \big(R+\gamma V_v(s')-V_v(s)\big)e \\
      \end{align}$$<br/>
      <p>where</p>
      $$e\leftarrow \lambda e+\nabla_\theta\ln\pi(a|s,\theta)$$<br/>
      <p>is the eligibility trace.</p>

      <h2 id="Summary-of-First-Order-Gradient-Methods">Summary of First-Order Gradient Methods</h2>
      <table style="width:120%;">
        <colgroup>
        <col width="10%" />
        <col width="40%" />
        <col width="10%" />
        <col width="10%" />
        <col width="10%" />
        <col width="10%" />
        <col width="10%" />
        <col width="10%" />
        </colgroup>
        <thead>
        <tr class="header">
        <th align="left">Name</th>
        <th align="left">Formula</th>
        <th align="center">Baseline</th>
        <th align="center">Episodic</th>
        <th align="center">Continuing</th>
        <th align="center">Parameters</th>
        <th align="center">Variance</th>
        <th align="center">Bias</th>
        </tr>
        </thead>
        <tbody>
        <tr class="odd">
        <td align="left">Monte-Carlo (REINFORCE)</td>
        <td align="left"><span class="math inline">\(\nabla_\theta J(\theta)\approx g_t\nabla_\theta\ln\pi(a|s,\theta)\)</span></td>
        <td align="center">No</td>
        <td align="center">Yes</td>
        <td align="center">No</td>
        <td align="center">1</td>
        <td align="center">High</td>
        <td align="center">Low</td>
        </tr>
        <tr class="even">
        <td align="left">Actor-Critic</td>
        <td align="left"><span class="math inline">\(\nabla_\theta J(\theta)\approx Q_w(a,s)\nabla_\theta\ln\pi(a|s,\theta)\)</span></td>
        <td align="center">No</td>
        <td align="center">Yes</td>
        <td align="center">Yes</td>
        <td align="center">2</td>
        <td align="center">Medium</td>
        <td align="center">Medium (but can be Low)</td>
        </tr>
        <tr class="odd">
        <td align="left">Monte Carlo with Baseline</td>
        <td align="left"><span class="math inline">\(\nabla_\theta J(\theta)\approx \big(g-V_v(s)\big)\nabla_\theta\ln\pi(a|s,\theta)\)</span></td>
        <td align="center">Yes</td>
        <td align="center">Yes</td>
        <td align="center">No</td>
        <td align="center">2</td>
        <td align="center">Medium</td>
        <td align="center">Low</td>
        </tr>
        <tr class="even">
        <td align="left">Advantage Actor-Critic</td>
        <td align="left"><span class="math inline">\(\nabla_\theta J(\theta)\approx \big(Q_w(a,s)-V_v(s)\big)\nabla_\theta\ln\pi(a|s,\theta)\)</span></td>
        <td align="center">Yes</td>
        <td align="center">Yes</td>
        <td align="center">Yes</td>
        <td align="center">3</td>
        <td align="center">Low</td>
        <td align="center">Medium (but can be Low)</td>
        </tr>
        <tr class="odd">
        <td align="left">TD Actor-Critic</td>
        <td align="left"><span class="math inline">\(\nabla_\theta J(\theta)\approx \big(R+\gamma V_v(s&#39;)-V_v(s)\big)\nabla_\theta\ln\pi(a|s,\theta)\)</span></td>
        <td align="center">Yes</td>
        <td align="center">Yes</td>
        <td align="center">Yes</td>
        <td align="center">2</td>
        <td align="center">Low</td>
        <td align="center">Medium</td>
        </tr>
        <tr class="even">
        <td align="left">TD(<span class="math inline">\(\lambda\)</span>) Actor-Critic (Forward)</td>
        <td align="left"><span class="math inline">\(\nabla_\theta J(\theta)\approx \big(g^\lambda-V_v(s)\big)\nabla_\theta\ln\pi(a|s,\theta)\)</span></td>
        <td align="center">Yes</td>
        <td align="center">Yes</td>
        <td align="center">No</td>
        <td align="center">2</td>
        <td align="center">Low-Medium (tradeoff)</td>
        <td align="center">Low-Medium (tradeoff)</td>
        </tr>
        <tr class="odd">
        <td align="left">TD(<span class="math inline">\(\lambda\)</span>) Actor-Critic (Backward)</td>
        <td align="left"><span class="math inline">\(\nabla_\theta J(\theta)\approx \big(R+\gamma V_v(s&#39;)-V_v(s)\big)e\)</span> <br> <span class="math inline">\(e\leftarrow \lambda e+\nabla_\theta\ln\pi(a|s,\theta)\)</span></td>
        <td align="center">Yes</td>
        <td align="center">Yes</td>
        <td align="center">Yes</td>
        <td align="center">2</td>
        <td align="center">Low-Medium (tradeoff)</td>
        <td align="center">Low-Medium (tradeoff)</td>
        </tr>
        </tbody>
        </table>

      <h1 id="Natural-Gradient-Methods">Natural Gradient Methods</h1>
      <p>Firstly, we show some properties of the KL divergence. Secondly, we derive the update rule of natural gradient algorithms. Thirdly, we derive the natural policy gradient algorithm.</p>
      
      <h2 id="Properties-of-KL-Divergence">Properties of KL Divergence</h2>
      <p>We try to use similar notation used so far. For example, we use $\pi(a|\theta)$ to refer to $\pi(a|s,\theta)$ in the context of policy gradient algorithms. More generally, we can think of $\pi(a|\theta)$ to any arbitrary density parameterized by vector $\theta$.
      We show that KL divergence can be approximated in the following way:</p>
      <blockquote><p>$$\begin{align}
          KL\big\{\pi_{\theta_t}\|\pi_\theta\} &amp;\approx \Delta\theta^TG(\theta_t)\Delta\theta \\
          G(\theta) &amp;= E_A\big\{\nabla_\theta\ln\pi(A|\theta)\nabla_\theta\ln\pi(A|\theta)^T\big\} \\
          \Delta\theta &amp;= \theta-\theta_t
      \end{align}$$</p>
      </blockquote>
      <p>for $\|\Delta\theta\|\rightarrow 0$.
      <br>Let's derive this approximation ($\pi_\theta(\cdot)\doteq\pi(\cdot|\theta)$)</p>
      $$\begin{align}
          KL\big\{\pi_{\theta_t}\|\pi_\theta\} &amp;= \int \pi_{\theta_t}(a)\ln\frac{\pi_{\theta_t}(a)}{\pi_\theta(a)}da \\
                                               &amp;=-\int \pi_{\theta_t}(a)\ln\frac{\pi_\theta(a)}{\pi_{\theta_t}(a)}da \\
                                               &amp;=-\int \pi_{\theta_t}(a)\ln\frac{\Delta p(a)+\pi_{\theta_t}(a)}{\pi_{\theta_t}(a)}da \quad\text{where }\Delta p=\pi_\theta-\pi_{\theta_t} \\
                                               &amp;=-\int \pi_{\theta_t}(a)\ln\bigg\{1+\frac{\Delta p(a)}{\pi_{\theta_t}(a)}\bigg\}da \\
                                               &amp;\approx-\int \pi_{\theta_t}(a)\bigg\{\frac{\Delta p(a)}{\pi_{\theta_t}(a)}-\frac{1}{2}\bigg[\frac{\Delta p(a)}{\pi_{\theta_t}(a)}\bigg]^2\bigg\}da \quad\text{where we use Taylor expansion }\ln(1+x)=x-\frac{x^2}{2}+O(x^2)\\
                                               &amp;=-\int \pi_{\theta_t}(a)\frac{\Delta p(a)}{\pi_{\theta_t}(a)}da+\frac{1}{2}\int \pi_{\theta_t}(a)\bigg[\frac{\Delta p(a)}{\pi_{\theta_t}(a)}\bigg]^2da \\
                                               &amp;=-\int \Delta p(a)da+\frac{1}{2}\int \pi_{\theta_t}(a)\bigg[\frac{\Delta p(a)}{\pi_{\theta_t}(a)}\bigg]^2da \\
                                               &amp;\approx-\int \nabla_\theta\pi_{\theta_t}(a)^T\Delta\theta da+\frac{1}{2}\int \pi_{\theta_t}(a)\bigg[\frac{\Delta p(a)}{\pi_{\theta_t}(a)}\bigg]^2da \quad\text{note that }\Delta p(a)\approx\nabla_\theta\pi_{\theta_t}(a)^T\Delta\theta\\
                                               &amp;=-\bigg[\int \nabla_\theta\pi_{\theta_t}(a)^T da\bigg]\Delta\theta+\frac{1}{2}\int \pi_{\theta_t}(a)\bigg[\frac{\Delta p(a)}{\pi_{\theta_t}(a)}\bigg]^2da \\
                                               &amp;=-\bigg[\int \nabla_\theta\pi_{\theta_t}(a) da\bigg]^T\Delta\theta+\frac{1}{2}\int \pi_{\theta_t}(a)\bigg[\frac{\Delta p(a)}{\pi_{\theta_t}(a)}\bigg]^2da \\
                                               &amp;=-\bigg[\nabla_\theta\int \pi_{\theta_t}(a) da\bigg]^T\Delta\theta+\frac{1}{2}\int \pi_{\theta_t}(a)\bigg[\frac{\Delta p(a)}{\pi_{\theta_t}(a)}\bigg]^2da \\
                                               &amp;=-\bigg[\nabla_\theta 1\bigg]^T\Delta\theta+\frac{1}{2}\int \pi_{\theta_t}(a)\bigg[\frac{\Delta p(a)}{\pi_{\theta_t}(a)}\bigg]^2da \\
                                               &amp;=\frac{1}{2}\int \pi_{\theta_t}(a)\bigg[\frac{\Delta p(a)}{\pi_{\theta_t}(a)}\bigg]^2da \\
                                               &amp;=\frac{1}{2}E_A\bigg\{\bigg[\frac{\Delta p(a)}{\pi_{\theta_t}(a)}\bigg]^2\bigg\} \\
                                               &amp;=\frac{1}{2}E_A\big\{\Delta\theta^T\nabla_\theta\ln\pi(A|\theta_t)\nabla_\theta\ln\pi(A|\theta_t)^T\Delta\theta\big\} \quad\text{where we use Taylor expansion as before }\frac{\Delta p(A)}{\pi_{\theta_t}(A)}\approx \frac{\nabla_\theta\pi_{\theta_t}(A)^T\Delta\theta}{\pi_{\theta_t}(A)}=\frac{\sum_i\frac{\partial\pi_{\theta_t}(A)}{\partial\theta_i}\Delta\theta_i}{\pi_{\theta_t}(A)}=\sum_i\frac{\partial\ln\pi_{\theta_t}(A)}{\partial\theta_i}\Delta\theta_i = \nabla_\theta\ln\pi_{\theta_t}(A)^T\Delta\theta = \nabla_\theta\ln\pi(A|\theta_t)^T\Delta\theta \\
                                               &amp;=\frac{1}{2}\Delta\theta^TE_A\big\{\nabla_\theta\ln\pi(A|\theta_t)\nabla_\theta\ln\pi(A|\theta_t)^T\big\}\Delta\theta \\
                                               &amp;=\frac{1}{2}\Delta\theta^TG(\theta_t)\Delta\theta
      \end{align}$$<br/>

      <h2 id="Update-Rule-for-Natural-Gradient-Algorithms">Update Rule for Natural Gradient Algorithms</h2>
      <p>The update rule is derived from the following constrained optimization problem</p>
      $$\begin{align}
          \theta_{t+1} = \arg &amp; \min_\theta -J(\theta) \\
                             &amp; \text{s. t. } \frac{1}{2}\Delta\theta^TG(\theta_t)\Delta\theta\leq\epsilon_t
      \end{align}$$<br/>
      <p>Let's consider the Lagrangian</p>
      $$\begin{align}
      \mathcal{L}(\theta,\lambda_t) &amp;= -J(\theta) + \lambda_t\bigg[\frac{1}{2}\Delta\theta^TG(\theta_t)\Delta\theta-\epsilon_t\bigg] \\
                                    &amp;\approx -J(\theta_t) - \nabla_\theta J(\theta_t)^T\Delta\theta + \lambda_t\bigg[\frac{1}{2}\Delta\theta^TG(\theta_t)\Delta\theta-\epsilon_t\bigg] \\
                                    &amp;\quad\quad\text{where we use the Taylor approximation for }J(\theta) \\
                                    &amp;= - J(\theta_t) - \nabla_\theta J(\theta_t)^T(\theta-\theta_t) + \lambda_t\bigg[\frac{1}{2}(\theta-\theta_t)^TG(\theta_t)(\theta-\theta_t)-\epsilon_t\bigg] \\
      \end{align}$$<br/>
      <p>Now, let's compute the gradient with respect to $\theta$ and equate it to 0.</p>
      $$\begin{align}
          \nabla_\theta\mathcal{L}(\theta,\lambda_t) &amp;\approx -\nabla_\theta J(\theta_t) + \lambda_tG(\theta_t)(\theta-\theta_t)=0
      \end{align}$$<br/>
      <p>which give us the update rule</p>
      <blockquote><p>$$\theta=\theta_t+\frac{G(\theta_t)^{-1}}{\lambda_t}\nabla_\theta J(\theta_t)$$</p>
      <p>Therefore,</p>
      $$\nabla_\theta^{nat}J(\theta_t)=G(\theta_t)^{-1}\nabla_\theta J(\theta_t)$$<br/>
      </blockquote>

      <h2 id="Natural-Policy-Gradient-Algorithm">Natural Policy Gradient Algorithm</h2>
      <blockquote><p>Given an action value approximator $Q_w(a,s)=\nabla_\theta\ln\pi(a|s,\theta)^Tw$ (which is called <strong>compatible function approximator</strong>, as $\nabla_wQ_w(a,s)=\nabla_\theta\ln\pi(a|s,\theta)$), we have that</p>
      $$\nabla_\theta^{nat}J(\theta_t)\propto w_t$$<br/>
      </blockquote>
      <p>In fact, assume that $Q_w(a,s)=q_{\pi_\theta}(a,s)$</p>
      $$\begin{align}
          \nabla_\theta J(\theta) &amp;\propto E\big\{q_{\pi\theta}(A,S)\nabla_\theta\ln\pi(A|S,\theta)\big\} \\
                                  &amp;= E\big\{\nabla_\theta\ln\pi(A|S,\theta)Q_w(A,S)\big\} \\
                                  &amp;= E\big\{\nabla_\theta\ln\pi(A|S,\theta)\nabla_\theta\ln\pi(a|s,\theta)^Tw\big\} \\
                                  &amp;= E\big\{\nabla_\theta\ln\pi(A|S,\theta)\nabla_\theta\ln\pi(a|s,\theta)^T\big\}w \\
                                  &amp;= G(\theta)w \\
      \end{align}$$<br/>
      <p>Now by the result of previous section</p>
      $$\begin{align}
          \nabla_\theta^{nat}J(\theta_t) &amp;= G(\theta_t)^{-1}\nabla_\theta J(\theta_t) \\
                                         &amp;\propto G(\theta_t)^{-1}G(\theta_t)w_t \\
                                         &amp;\propto w_t
      \end{align}$$<br/>
      <p><strong>Therefore, using a compatible function approximator allows to avoid to compute $G(\theta_t)$ and its inverse</strong>.</p>

      
    </div><!-- /.article-wrap -->

    <div id="disqus_thread" style="width:140%;text-align:justify;margin-top:3cm;margin-bottom:2cm;"></div>
    <script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://ema87.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
    </script>
            
  </article>
</div><!-- /#index -->
                    

<div class="footer-wrap">
  <footer>
<span>&copy; 2020 Emanuele Sansone.

  </span></footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="https://emsansone.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="https://emsansone.github.io/assets/js/scripts.min.js"></script>

</body></html>
