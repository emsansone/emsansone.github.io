<!DOCTYPE html>

<head>
<!-- saved from url=(0028) https://emsansone.github.io/ -->
<!-- <html class=" js no-touch rgba hsla textshadow opacity svg" lang="en"><!--<![endif]-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Emanuele Sansone</title>
<meta name="description" content="Emanuele Sansone">
<meta name="keywords" content="Jekyll, theme, responsive, blog, template">


<!-- Twitter Cards -->
<meta name="twitter:title" content="Emanuele Sansone">
<meta name="twitter:description" content="Emanuele Sansone">



<!--<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://emsansone.github.io/img/Background3.jpg">-->

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Emanuele Sansone">
<meta property="og:description" content="Emanuele Sansone">
<meta property="og:url" content="https://emsansone.github.io/blog/">
<meta property="og:site_name" content="Emanuele Sansone">





<link rel="canonical" href="https://emsansone.github.io/blog/">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="https://emsansone.github.io/assets/css/main.css">
    
<meta http-equiv="cleartype" content="on">
        
<!-- Modernizr -->
<script src="https://emsansone.github.io/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>
        
<link href='//fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700%7CPT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>


<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="https://emsansone.github.io/img/logo.png">
<!-- 32x32 -->
<link rel="shortcut icon" href="https://emsansone.github.io/img/logo-32x32.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="https://emsansone.github.io/img/logo-57x57.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://emsansone.github.io/img/logo-72x72.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://emsansone.github.io/img/logo-114x114.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://emsansone.github.io/img/logo-144x144.png">

<style>
  table, th, td {
    border: 1px solid grey;
    border-collapse: collapse;
  }
</style>

</head>

<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    // Center justify equations in code and markdown cells. Elsewhere
    // we use CSS to left justify single line equations in code cells.
    displayAlign: 'center',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}},
        linebreaks: { automatic: true }
    }
});
</script>

<body class="page">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="https://emsansone.github.io/">Website</a>
	</div><!-- /.site-name -->
    <div class="top-navigation">
        <nav role="navigation" id="site-nav" class="nav">
		    <ul>
              <li><a href="https://emsansone.github.io/blog/rl/rl0.htm">Index</a></li>	        					    
					    <li><a href="https://emsansone.github.io/blog/rl/rl3.htm" >Previous Lecture</a></li>
					    <li><a href="https://emsansone.github.io/blog/rl/rl5.htm" >Next Lecture</a></li>

		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->


<!--<div class="image-wrap">
    <img src="https://emsansone.github.io/img/Background3.jpg" alt="Emanuele Sansone feature image">
</div>-->

<div id="main" role="main">
  <div class="article-author-side">
    <img src="https://emsansone.github.io/img/Me.jpg" class="bio-photo" alt="Emanuele Sansone bio photo">

    <h2>Emanuele Sansone</h2>
    <p>PhD in machine learning and artificial intelligence.</p>
    <a href="mailto:e.sansone@hotmail.it" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/email.png" height=14px width=14px> Email</a>

    <a href="https://linkedin.com/in/emanuele-sansone-97329475" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/linkedin.png" height=14px width=14px> LinkedIn</a>

    <a href="https://twitter.com/skiera87" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/twitter.png" height=14px width=14px> Twitter</a>

    <!-- <a href="https://github.com/emsansone" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/github.png" height=14px width=14px> Github</a> -->
  </div>

  <article class="page">

    <div class="article-wrap" style="width:140%;text-align:justify;">
    
      <h1 id="Lecture-4:-Model-Free-Prediction">Model-Free Prediction</h1>
      <p><strong>Assumption:</strong></p>
      <ol>
      <li><strong>unknown MDP</strong>. You know the <strong>states</strong>, but you don't know the transition probabilities and the reward function</li>
      <li><strong>All states</strong> must be visited <strong>enough</strong> times.</li>
      </ol>
      <p><strong>Goal</strong>. We want to perform policy evaluation. Recall that we have $v_\pi(s)=E_{\pi}\{G_t|S_t=s\}$ and $G_t=R_{t+1}+\gamma R_{t+2} + ... + \gamma^{T-1}R_T$.</p>

      <h2 id="Solution-1:-Monte-Carlo---update-after-batch-of-episodes---(update-only-for-first-visit-of-state)">Solution 1: Monte-Carlo - update after batch of episodes - (update only for first visit of state)</h2>
      <blockquote><p><strong>PSEUDOCODE</strong></p>
      <ul>
      <li>Given <strong>a batch of episodes</strong>.<ul>
      <li>Count the number of times a state is visited <strong>for the first time in an episode</strong> , i.e. $N(s)$.</li>
      <li><strong>At the end of each episode</strong> compute the <strong>cumulative return</strong> for those visits, i.e. $S(s)$.</li>
      </ul>
      </li>
      <li><strong>After the whole batch of episodes</strong>, compute $v(s)=\frac{S(s)}{N(s)}$.</li>
      </ul>
      </blockquote>
      <p><strong>State-value function is estimated using empirical mean</strong>, namely $G_t=\frac{1}{N}\sum_{i=1}^N G_{t,i}$.<br> $G_t$ a Gaussian RV by central limit theorem and variance decreases as $1/N$.</p>

      <h2 id="Solution-2:-Monte-Carlo---update-after-batch-of-episodes---(update-for-every-visit-of-state)">Solution 2: Monte-Carlo - update after batch of episodes - (update for every visit of state)</h2>
      <blockquote><p><strong>PSEUDOCODE</strong></p>
      <ul>
      <li>Given <strong>a batch of episodes</strong>.<ul>
      <li>Count the number of times a state is visited, i.e. $N(s)$.</li>
      <li><strong>At the end of each episode</strong> compute the <strong>cumulative return</strong> for those visits, i.e. $S(s)$.</li>
      </ul>
      </li>
      <li><strong>After the whole batch of episodes</strong>, compute $v(s)=\frac{S(s)}{N(s)}$.</li>
      </ul>
      </blockquote>

      <h2 id="Solution-3:-Incremental-Monte-Carlo---update-after-an-episode">Solution 3: Incremental Monte-Carlo - update after an episode</h2>
      <p>Note that given a sequence $x_1,x_2,\dots$, we can express the mean estimate in the following way:
      $$\begin{align}\mu_k &amp;= \frac{1}{k}\sum_{i=1}^kx_i \\
                           &amp;= \frac{1}{k}\big(x_k + \sum_{i=1}^{k-1}x_i\big) \\
                           &amp;= \frac{1}{k}\big(x_k + (k-1)/(k-1)\sum_{i=1}^{k-1}x_i\big) \\
                           &amp;= \frac{1}{k}\big(x_k + (k-1)\mu_{k-1}\big) \\
                           &amp;= \mu_{k-1} + \frac{1}{k}\big(x_k -\mu_{k-1}\big)
                           \end{align}$$</p>
      <blockquote><p><strong>PSEUDOCODE</strong></p>
      <ul>
      <li>Given <strong>a batch of episodes</strong>.<ul>
      <li>Count the number of times a state is visited (First/Every) , i.e. $N(s)$.</li>
      <li><strong>At the end of an episode</strong> compute $v(s)=v(s)+\frac{1}{N(s)}\big(G_t - v(s)\big)$</li>
      </ul>
      </li>
      </ul>
      </blockquote>
      <p>For <strong>non-stationary</strong> environment the update equation can be $v(s)=v(s)+\alpha\big(G_t - v(s)\big)$. Nevertheless, this is the solution used in practice.</p>

      <h2 id="Solution-4:-Temporal-Difference-Learning----complete-online-algorithm">Solution 4: Temporal-Difference Learning  - complete online algorithm</h2>
      <p>Note that</p>
          <span class="math display">\[\begin{align}v_\pi(s) &amp;= E_\pi\{G_t|S_t=s\} \\
            &amp;=\sum_{a}\pi(a|s)\big(R_s^a + \gamma \sum_{s&#39;}P_{ss&#39;}^a E_\pi\{G_{t+1}|S_{t+1}=s&#39;\}\big) \\
            &amp;=\sum_{a}\pi(a|s)\big(R_s^a + \gamma \sum_{s&#39;}P_{ss&#39;}^a v_\pi(s&#39;)\big)\end{align}\]</span><br/>
      <p>Therefore, if you observe $S_t,A_t,S_{t+1}$, then you can approximate $v(S_t)$ as $R_{t+1} + \gamma v(S_{t+1})$.</p>
      <blockquote><p><strong>PSEUDOCODE</strong></p>
      <ul>
      <li><strong>After each single visit</strong> compute $v(S_t)=v(S_t)+\frac{1}{N(s)}\big([R_{t+1}+\gamma v(S_{t+1})] - v(S_t)\big)$.</li>
      </ul>
      </blockquote>
      <p>$R_{t+1}+\gamma v(S_{t+1})$ is called <strong>TD target</strong> and $[R_{t+1}+\gamma v(S_{t+1})] - v(s)$ is called <strong>TD error</strong>.</p>
      <br><strong>Considerations (vs. Monte-Carlo)</strong></br>
      <ol>
      <li>TD can learn <strong>without final outcome</strong> (non-terminating or incomplete sequences)</li>
      <li>TD can learn <strong>online</strong></li>
      <li>TD target has <strong>bias</strong> (because $v(S_{t+1})$ is a biased estimate of $v_\pi(S_{t+1})$) - MC is unbiased ($G_{t}$ is an unbiased of $v_\pi(S_t)$)</li>
      <li>TD target has <strong>low variance</strong> (because it depends only on 1 action, 1 reward and 1 transition) - MC has high varaince (because it depends on all random subsequent steps).</li>
      <li>Because of low variance, TD usually <strong>converges faster</strong> than MC.</li>
      <li>TD converges to $v_\pi(s)$ (except in some cases when you use function approximation).</li>
      <li>Because of bias, TD is sensitive to <strong>the initial starting point</strong>.</li>
      </ol>

      <h2 id="Analysis-of-Monte-Carlo-and-Temporal-Difference">Analysis of Monte-Carlo and Temporal Difference</h2>
      <p><strong>Question 1</strong>. Both MC and TD converge to the true value (when the number of episodes is large enough and you visit all states). <strong>What happens when the number of episodes is finite?</strong><br>
      <strong>Answer</strong>. They can converge to different solutions.<br>
      Example: Consider a batch of episodes with 2 states, A and B (no discounting)</p>
      <ul>
      <li>B,1</li>
      <li>B,1</li>
      <li>B,1</li>
      <li>B,1</li>
      <li>B,1</li>
      <li>B,1</li>
      <li>B,0</li>
      <li>A,0,B,0</li>
      </ul>
      <p>MC achieves $V(A)=0$ and $V(B)=6/8=3/4$, while TD achieves $V(A)=V(B)=3/4$.
        In fact, MC is the <strong>solution of the squared error objective</strong> $\sum_{e=episodes}\sum_{t=visit}(G_t^e-v(S_t^e))^2$, while TD <strong>first creates a Markov model and then solves the model</strong>. In fact, it implicitly estimates the transition probabilities and the reward function:</p>
      $$\hat{P}_{ss'}^a=\frac{1}{N(a,s)}\sum_{e=episodes}\sum_{t=visit}1[S_t^e,A_t^e,S_{t+1}^e=s,a,s']$$<br/>
      $$\hat{R}_s^a=\frac{1}{N(a,s)}\sum_{e=episodes}\sum_{t=visit}1[S_t^e,A_t^e = s,a]R_{t+1}^e$$<br/>
      <p><strong>This tells us that TD exploits the Markov property and it typically works better in Markov environments, while MC works better in non-Markov environments (like POMDPs)</strong>.</p>
      <br><strong>Question 2</strong>. Can we unify the methods?</br>
      <p><strong>Answer</strong>. Yes, let's consider Dynamic Programming, Monte-Carlo and Temporal Difference.</p>
      <img src="./img/Comparison.png" alt="Comparison"><br/><br/>
      <p>Define <strong>bootstrapping</strong> (making only 1 step ahead) and <strong>sampling</strong> (estimate the expectation)</p><br/>
      <p style="text-align:center;"><img src="./img/View.png" alt="View" style="width:50%;height:50%;"></p><br/>
      <p>We can unify Monte-Carlo and Temporal Difference, by considering <strong>intermediate look aheads</strong>.
      Define <strong>n-step return</strong>, namely $G_t^{(n)}=R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{n-1}R_{t+n}+\gamma^nv(S_{t+n})$</p>
      <blockquote><p><strong>PSEUDOCODE (n-step TD)</strong></p>
      <ul>
      <li><strong>After each single visit</strong> compute $v(S_t)=v(S_t)+\alpha\big(G_t^{(n)} - v(S_t)\big)$.</li>
      </ul>
      </blockquote>
      <p>If $n=1$ we get normal TD, if $n\rightarrow\infty$ we get MC.
      <br><strong>Which $n$ is the best?</strong> Combine all!
      <br>(<strong>Geometric average</strong>) Define the <strong>$\lambda$-return</strong>, namely $G_t^\lambda=(1-\lambda)\sum_{n=1}^\infty\lambda^{n-1}G_t^{(n)}$.</p><br/>
      <p style="text-align:center;"><img src="./img/Average.png" alt="Average" style="width:25%;height:25%;"></p><br/>
      <blockquote><p><strong>PSEUDOCODE (TD($\lambda$))</strong></p>
      <ul>
      <li><strong>After each episode</strong> compute $v(S_t)=v(S_t)+\alpha\big(G_t^{\lambda} - v(S_t)\big)$.</li>
      </ul>
      </blockquote>
      <p><strong>Question 3</strong>. How is it possible to run TD($\lambda$) online instead of waiting until the end of an episode like in incremental Monte-Carlo?
      <br><strong>Answer</strong>. Define an <strong>eligibility trace</strong>, namely</p>
      $$E_0(s)=0$$<br/>
      $$E_t(s)=\gamma\lambda E_{t-1}(s) + 1[S_t=s]$$<br/>
      <p style="text-align:center;"><img src="./img/Eligibility.png" alt="Eligibility" style="width:60%;height:60%;"></p><br/>
      <p>We can rewrite TD($\lambda$) in an online fashion</p>
      <blockquote><p><strong>PSEUDOCODE (online TD($\lambda$))</strong></p>
      <ul>
      <li><strong>After each single visit</strong> compute $v(S_t)=v(S_t)+\alpha \delta_t E_t(s)$, where $\delta_t$ is the TD error, namely $\delta_t=R_{t+1}+\gamma v(S_{t+1})-v(S_t)$</li>
      </ul>
      </blockquote>
      <p><strong>Theorem</strong>. Assume you perform <strong>offline updates</strong> (namely, for online TD($\lambda$) you update the state-value function at the end of each episode). Then, the total update performed by online TD($\lambda$) is equivalent to the total update performed by TD($\lambda$). In fact,</p>
      $$\alpha\sum_{t=1}^T \delta_t E_t(s)=\alpha\sum_{t=1}^T\big(G_t^{\lambda}-v(S_t)\big)1[S_t=s]$$<br/>
      <p><strong>Proof</strong>. We prove it just for the case in which state $s$ is visited once in the episode. Let's assume that the visit occurs at time $k$.
      <br>Recall the definition of eligibility trace, viz. $E_t(s)=\lambda\gamma E_{t-1}(s) + 1[s_t=s]$ and $E_0(s)=0$.
      Therefore, we have that for $t&lt;k$, $E_t(s)=0$ and for $t\leq k$, $E_t(s)=(\lambda\gamma)^{t-k}$.
      Consequently, $\alpha\sum_{t=1}^T \delta_t E_t(s)=\alpha\sum_{t=k}^T \delta_t (\lambda\gamma)^{t-k}$. 
      Furthermore, $\alpha\sum_{t=1}^T\big(G_t^{\lambda}-v(S_t)\big)1[S_t=s]=\alpha \big(G_k^{\lambda}-v(S_k)\big)$.
      <br>Therefore, we need to prove the following statement</p>
      $$\alpha\sum_{t=k}^T \delta_t (\lambda\gamma)^{t-k}=\alpha \big(G_k^{\lambda}-v(S_k)\big)$$<br/>
      <p>We prove this for $T=k+1$, namely</p>
      $$\sum_{t=k}^{k+1} \delta_t (\lambda\gamma)^{t-k}= \big(G_k^{\lambda}-v(S_k)\big)$$<br/>
      <p>Now</p>
      $$\begin{align}\sum_{t=k}^{k+1} \delta_t (\lambda\gamma)^{t-k} &amp;=\delta_k + \delta_{k+1}\lambda\gamma \\
                  &amp;= R_{k+1} + \gamma v(S_{k+1}) - v(S_k) + \lambda\gamma\big(R_{k+2} + \gamma v(S_{k+2}) - v(S_{k+1})\big) \\
                  &amp;= R_{k+1} + \gamma v(S_{k+1}) - \lambda\gamma v(S_{k+1}) + \lambda\gamma\big(R_{k+2} + \gamma v(S_{k+2})\big) - v(S_k) \quad\text{ and now add }\quad -\lambda R_{k+1} + \lambda R_{k+1} \\
                  &amp;= R_{k+1} + \gamma v(S_{k+1}) -\lambda R_{k+1} + \lambda R_{k+1} - \lambda\gamma v(S_{k+1}) + \lambda\gamma\big(R_{k+2} + \gamma v(S_{k+2})\big) - v(S_k) \\ 
                  &amp;= R_{k+1} + \gamma v(S_{k+1}) -\lambda R_{k+1} - \lambda\gamma v(S_{k+1}) + \lambda R_{k+1} + \lambda\gamma\big(R_{k+2} + \gamma v(S_{k+2})\big) - v(S_k) \\
                  &amp;= R_{k+1} + \gamma v(S_{k+1}) -\lambda \big(R_{k+1} + \gamma v(S_{k+1})\big) + \lambda R_{k+1} + \lambda\gamma\big(R_{k+2} + \gamma v(S_{k+2})\big) - v(S_k) \\
                  &amp;= \big(1-\lambda\big)\big(R_{k+1} + \gamma v(S_{k+1})\big) + \lambda R_{k+1} + \lambda\gamma\big(R_{k+2} + \gamma v(S_{k+2})\big) - v(S_k) \\
                  &amp;= \big(1-\lambda\big)\big(R_{k+1} + \gamma v(S_{k+1})\big) + \lambda\big(R_{k+1} + \gamma R_{k+2} + \gamma^2 v(S_{k+2})\big) - v(S_k) \\
                  &amp;= \big(1-\lambda\big)G_k^{(1)} + \lambda G_k^{(2)} - v(S_k) \\
                  &amp;= G_k^\lambda - v(S_k) \end{align}$$<br/>
      <p><strong>QED</strong></p>
      <br><strong>Final considerations</strong>.
      <ol>
      <li>if $\lambda=0$, then $G_t^{\lambda}=(1-\lambda)\sum_{n=1}^\infty\lambda^{n-1}G_t^{(n)}=G_t^{(1)}$. In other words, <strong>we recover TD</strong>.</li>
      <li>if $\lambda=1$, then all terms except the last one in the summation of $G_t^{\lambda}$ cancel out. Therefore, we obtain $G_t^{\lambda}=G_t^{(T)}$, namely <strong>we recover MC</strong>.</li>
      <li>Note that the paper of <a href="http://proceedings.mlr.press/v32/seijen14.pdf">Sutton and von Seijen (ICML 2014)</a>, proposes a slightly different eligibility trace and prove the equivalence in the theorem even for <strong>online updates</strong>.</li>
      </ol>
    </br>
      
    </div><!-- /.article-wrap -->

    <div id="disqus_thread" style="width:140%;text-align:justify;margin-top:3cm;margin-bottom:2cm;"></div>
    <script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://ema87.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
    </script>
            
  </article>
</div><!-- /#index -->
                    

<div class="footer-wrap">
  <footer>
<span>&copy; 2020 Emanuele Sansone.

  </span></footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="https://emsansone.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="https://emsansone.github.io/assets/js/scripts.min.js"></script>

</body></html>
