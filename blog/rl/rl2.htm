<!DOCTYPE html>

<head>
<!-- saved from url=(0028) https://emsansone.github.io/ -->
<!-- <html class=" js no-touch rgba hsla textshadow opacity svg" lang="en"><!--<![endif]-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Emanuele Sansone</title>
<meta name="description" content="Emanuele Sansone">
<meta name="keywords" content="Jekyll, theme, responsive, blog, template">


<!-- Twitter Cards -->
<meta name="twitter:title" content="Emanuele Sansone">
<meta name="twitter:description" content="Emanuele Sansone">



<!--<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://emsansone.github.io/img/Background3.jpg">-->

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Emanuele Sansone">
<meta property="og:description" content="Emanuele Sansone">
<meta property="og:url" content="https://emsansone.github.io/blog/">
<meta property="og:site_name" content="Emanuele Sansone">





<link rel="canonical" href="https://emsansone.github.io/blog/">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="https://emsansone.github.io/assets/css/main.css">
    
<meta http-equiv="cleartype" content="on">
        
<!-- Modernizr -->
<script src="https://emsansone.github.io/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>
        
<link href='//fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700%7CPT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>


<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="https://emsansone.github.io/img/logo.png">
<!-- 32x32 -->
<link rel="shortcut icon" href="https://emsansone.github.io/img/logo-32x32.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="https://emsansone.github.io/img/logo-57x57.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://emsansone.github.io/img/logo-72x72.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://emsansone.github.io/img/logo-114x114.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://emsansone.github.io/img/logo-144x144.png">

</head>

<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    // Center justify equations in code and markdown cells. Elsewhere
    // we use CSS to left justify single line equations in code cells.
    displayAlign: 'center',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}},
        linebreaks: { automatic: true }
    }
});
</script>

<body class="page">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="https://emsansone.github.io/">Website</a>
	</div><!-- /.site-name -->
    <div class="top-navigation">
        <nav role="navigation" id="site-nav" class="nav">
		    <ul>
              <li><a href="https://emsansone.github.io/blog/rl/rl0.htm">Index</a></li>	        					    
					    <li><a href="https://emsansone.github.io/blog/rl/rl1.htm" >Previous Lecture</a></li>
					    <li><a href="https://emsansone.github.io/blog/rl/rl3.htm" >Next Lecture</a></li>

		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->


<!--<div class="image-wrap">
    <img src="https://emsansone.github.io/img/Background3.jpg" alt="Emanuele Sansone feature image">
</div>-->

<div id="main" role="main">
  <div class="article-author-side">
    <img src="https://emsansone.github.io/img/Me.jpg" class="bio-photo" alt="Emanuele Sansone bio photo">

    <h2>Emanuele Sansone</h2>
    <p>PhD in machine learning and artificial intelligence.</p>
    <a href="mailto:e.sansone@hotmail.it" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/email.png" height=14px width=14px> Email</a>

    <a href="https://linkedin.com/in/emanuele-sansone-97329475" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/linkedin.png" height=14px width=14px> LinkedIn</a>

    <a href="https://twitter.com/skiera87" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/twitter.png" height=14px width=14px> Twitter</a>

    <!-- <a href="https://github.com/emsansone" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/github.png" height=14px width=14px> Github</a> -->
  </div>

  <article class="page">

    <div class="article-wrap" style="width:140%;text-align:justify;">
    
      <h1 id="Lecture-2:-Markov-Decision-Processes">Markov Decision Processes</h1>
      <p>In this lecture, we provide the models for the <strong>environment</strong>.</p>

      <h2 id="Markov-Process">Markov Process</h2>
      <blockquote><p><strong>Markov Process</strong> is a tuple $&lt;S,P&gt;$, where</p>
      
      <ul>
      <li>$S$ is a finite set of states </li>
      <li>$P$ is the state transition probability matrix, namely</li><br/>
        <span class="math display">\[P=\left[\begin{array}P(S_{t+1}=s_1|S_t=s_1) &amp; \dots &amp; P(S_{t+1}=s_n|S_t=s_1) \\
          \vdots &amp; &amp; \vdots \\
          P(S_{t+1}=s_1|S_t=s_n) &amp; \dots &amp; P(S_{t+1}=s_n|S_t=s_n)
          \end{array}\right]\]</span>
      </ul>
      </blockquote>

      <ul>
      <li>The rows of transition matrix sum up to 1.</li>
      <li>A Markov Process can be visualized with a <strong>directed graph</strong> (nodes are states, there is also an absorbing state with infinite loop, and the edges are weightes according to the transition probability matrix).</li>
      <li>If $P$ changes over time, then you can convert back to <strong>stationary</strong> Markov process either by (i) adjusting over time or by (ii) adding more states.</li>
      </ul>

      <h2 id="Markov-Reward-Process">Markov Reward Process</h2>
      <blockquote><p><strong>Markov Reward Process</strong> is a tuple $&lt;S,P,R,\gamma&gt;$, where</p>
      <ul>
      <li>$\dots$</li>
      <li>$R$ is a reward function, in particular $R_s=E\{R_{t+1}|S_t=s\}$ (how much reward I get for state $s$) -<strong>the reward for a state can be stochastic, that's why you have expectation in this definition</strong></li>
      <li>$\gamma$ is a discount factor, namely $\gamma\in[0,1]$</li>
      </ul>
      </blockquote>
      <ul>
      <li>The discount factor is used to define the <strong>sample return</strong>, i.e. $G_t=R_{t+1} + \gamma R_{t+2} + \dots$</li>
      <li>$\gamma=0$ means that the return is focused on immediate return</li>
      <li>$\gamma=1$ requires that $\sum_{k=0}^{\infty}R_{t+k+1}&lt;\infty$. For example, by ensuring that the absorbing node has reward 0. (Need to be careful for cyclic Markov processes).</li>
      <li>A Markov Reward Process can be visualized with a <strong>directed graph</strong>.</li>
      <li>To model aninmal/human behaviour you can use <strong>hyperbolic discounting</strong>.</li>
      <li>The <strong>state value function</strong> is the <strong>expected return</strong> starting from state $s$, viz. $v(s)=E\{G_t|S_t=s\}$.</li>
      </ul>

      <h3 id="How-to-compute-the-state-value-function?">How to compute the state value function?</h3>
      <p>Note that we always have a <strong>sequence of random variables</strong>, i.e. $\dots,S_t,R_{t+1},S_{t+1},R_{t+2},\dots$</p>
      <ol>
      <li>By sampling an infinite sequence of states/rewards</li>
      <li>By using the <strong>Bellman equation</strong>, namely</li><br/>
        <span class="math display">\[\begin{align}v(s) &amp;= E\{G_t|S_t=s\} \\
          &amp;= E\{R_{t+1} + \gamma R_{t+2} + \dots|S_t=s\} \\
          &amp;= R_s + E\{\gamma R_{t+2} + \dots|S_t=s\} \text{ - by definition of conditional expectation} \\ 
          &amp;= R_s + \gamma E\{R_{t+2} + \gamma R_{t+3} + \dots|S_t=s\} \\
          &amp;= R_s + \gamma E\{G_{t+1}|S_t=s\} \\ 
          &amp;= R_s + \gamma \sum_{s&#39;}P_{ss&#39;}E\{G_{t+1}|S_{t+1}=s&#39;\} \\
          &amp;= R_s + \gamma \sum_{s&#39;}P_{ss&#39;}v(s&#39;)\end{align}\]</span>
      </ol>
      <p>Basically, we can rewrite it in matrix forms, namely $v=R+\gamma Pv$.</p>

      <h3 id="How-to-solve-the-Bellman-equation?">How to solve the Bellman equation?</h3><ol>
      <li>Direct computation by computing the <strong>inverse</strong> of $I-\gamma P$, but this is $O(n^3)$, where $n$ is the number of states. <strong>This is a solution for small MRPs</strong>.</li>
      <li>Using (i) <strong>dynamic programming</strong>, (ii) <strong>Monte Carlo</strong> evaluation (iii) <strong>temporal difference learning</strong>.</li>
      </ol>

      <h2 id="Markov-Decision-Process">Markov Decision Process</h2><blockquote><p><strong>Markov Decision Process</strong> is a tuple $&lt;S,A,P,R,\gamma&gt;$, where</p>
      <ul>
      <li>$\dots$</li>
      <li>$P$ is the state transition probability matrix, where each entry is defined as $P_{ss'}^a=P(S_{t+1}=s'|S_t=s, A_t=a)$</li>
      <li>$R$ is a reward function, in particular $R_s^a=E\{R_{t+1}|S_t=s, A_t=a\}$ (how much reward I get for state $s$)</li>
      </ul>
      </blockquote>
      <ul>
      <li>A Markov Decision Process can be visualized with a <strong>directed graph</strong> (i.e. you need to add a decision to each edge).</li>
      <li>A <strong>policy</strong> is a distribution over actions given states, namely $\pi(a|s)=P(A_t=a|S_t=s)$.</li>
      <li>There are <strong>two value functions</strong>:<ul>
      <li><strong>state-value function</strong> for a given policy $\pi$, which is the expected return starting from state $s$  and following policy $\pi$, namely $v_\pi(s)=E_\pi\{G_t|S_t=s\}$. Note that in this case, the expectation takes into account two sources of randomness, i.e. the <strong>randomness in the decision</strong> and the <strong>randomness in the state transition</strong>. (<strong>How good to be in particular state</strong>)</li>
      <li><strong>action-value function</strong> is the expected return starting from state $s$, choosing action $a$ and following policy $\pi$, namely $q_\pi(a,s)=E_\pi\{G_t|S_t=s, A_t=a\}$. (<strong>How good to be in particular state and to choose a specific action</strong>)</li>
      </ul>
      </li>
      </ul>

      <h3 id="How-to-compute-the-value-functions?">How to compute the value functions?</h3><p>Note that we always have a <strong>sequence of random variables</strong>, i.e. $\dots,S_t,A_t,R_{t+1},S_{t+1},A_{t+1},R_{t+2},\dots$</p>
      <p>As before, we can compute the value functions</p>
      <ul>
      <li>by sampling infinite sequence</li>
      <li>using <strong>Bellman Expectation Equations</strong>, namely:</li><br/>
        <span class="math display">\[\begin{align}v_\pi(s) &amp;= E_\pi\{G_t|S_t=s\} \\
          &amp;= \sum_{a}\pi(a|s)E_\pi\{G_t|S_t=s, A_t=a\} \text{ - by definition of conditional expectation}\\
          &amp;= \sum_{a}\pi(a|s)q_\pi(a,s)\end{align}\]</span>
      </ul>
      <p>$$\begin{align}q_\pi(a,s) &amp;= E_\pi\{G_t|S_t=s, A_t=a\} \\
                          &amp;= E_\pi\{R_{t+1} + \gamma R_{t+2} + \dots|S_t=s, A_t=a\} \\
                          &amp;= R_s^a + E_\pi\{\gamma R_{t+2} + \dots|S_t=s, A_t=a\} \text{ - by definition of conditional expectation}\\ 
                          &amp;= R_s^a + \gamma E_\pi\{R_{t+2} + \gamma R_{t+3} + \dots|S_t=s, A_t=a\} \\
                          &amp;= R_s^a + \gamma E_\pi\{G_{t+1}|S_t=s, A_t=a\} \\ 
                          &amp;= R_s^a + \gamma \sum_{s'}P_{ss'}^aE_\pi\{G_{t+1}|S_{t+1}=s'\} \\
                          &amp;= R_s^a + \gamma \sum_{s'}P_{ss'}^av_\pi(s')\end{align}$$</p>
      <p>Now, you can basically obtain the <strong>Bellman equation for the state-value function</strong>, namely:</p>
      $$v_\pi(s)=\sum_{a}\pi(a|s)\big(R_s^a + \gamma \sum_{s'}P_{ss'}^av_\pi(s')\big)$$<br/>
      and also the <strong>Bellman equation for the action-value function</strong>, namely:</p>
      $$q_\pi(a,s)=R_s^a + \gamma \sum_{s'}P_{ss'}^a\big(\sum_{a'}\pi(a'|s')q_\pi(a',s')\big)$$<br/>
      <p>The state-value function can be rewritten in the following way:</p>
      $$\begin{align}v_\pi(s)&amp;=\sum_{a}\pi(a|s)\big(R_s^a + \gamma \sum_{s'}P_{ss'}^av_\pi(s')\big) \\
                             &amp;= \sum_{a}\pi(a|s)R_s^a + \gamma\sum_{s'}\sum_{a}\pi(a|s)P_{ss'}^av_\pi(s') \\
                             &amp;= \sum_{a}\pi(a|s)R_s^a + \gamma\sum_{s'}\big(\sum_{a}\pi(a|s)P_{ss'}^a\big)v_\pi(s') \\
                             &amp;= R_s^\pi + \gamma\sum_{s'}P_{ss'}^\pi v_\pi(s')\end{align}$$<br/>
      <p>Therefore, the matrix form is $v_\pi=R^\pi + \gamma P^\pi v_\pi$.</p>

      <h3 id="How-to-compute-the-optimal-policy?">How to compute the optimal policy?</h3><p>In order to compute the optimal policy, we need a <strong>criterion</strong> which allows us to rank the policies.
      Therefore, we can use the state-value (and action-value) function(s).</p>
      <blockquote><p><strong>Maximum value functions</strong>. They are
      $$v^*(s)=\max_\pi v_\pi(s)$$ for every $s$.
      $$q^*(a,s)=\max_\pi q_\pi(a,s)$$ for every $s$.
      respectively</p>
      </blockquote>
      <p>Now, we can define the following criterion</p>
      <blockquote><p>Policy $\pi$ is <strong>better</strong> than policy $\pi'$ iff for every $s$ we have that $v_\pi(s)\geq v_{\pi'}(s)$.</p>
      </blockquote>
      <p>Therefore, the optimal policy is the one such that $\pi^*=arg\max_\pi v_\pi(s)$ for all $s$.</p>

      <h4 id="Does-the-optimal-policy-exist?">Does the optimal policy exist?</h4>
      <p><a href="https://www.wiley.com/en-us/Markov+Decision+Processes%3A+Discrete+Stochastic+Dynamic+Programming-p-9781118625873">Puterman</a>. There exists at least an optimal policy. And the optimal policy/ies achieve same values of the solution/s of the Bellman Optimal Equations.</p>
      <p>Proof hints: define Bellman Optimality Equation $\tilde{v}(s)=\max_a R_s^a + \gamma\sum_{s'}P_{ss'}^a\tilde{v}(s')$. Show that this operator is a contraction mapping, then you can use the Banach fixed-point theorem and state that the solution is unique $\tilde{v}(s)$. Now, by using Theorem 6.2.2 in <a href="https://www.wiley.com/en-us/Markov+Decision+Processes%3A+Discrete+Stochastic+Dynamic+Programming-p-9781118625873">Puterman</a>, you can prove that $\tilde{v}^*(s)=v^*(s)$.</p>

      <h4 id="Bellman-Optimality-Equations">Bellman Optimality Equations</h4>
      <p>Therefore, we have the <strong>Bellman Optimality Equation</strong> for <strong>the state-value function</strong></p>
      $$v^*(s)=\max_a R_s^a + \gamma\sum_{s'}P_{ss'}^av^*(s')$$
      <p>and note that</p>
      $$q^*(a,s)=\max_\pi q_\pi(a,s)=R_s^a + \gamma\sum_{s'}P_{ss'}^a\max_a v_\pi(s')=R_s^a + \gamma\sum_{s'}P_{ss'}^av^*(s')$$<br/>
      <p>Therefore, substituting this last equation into the Bellman Optimality Equation, we get</p>
      $$v^*(s)=\max_a q^*(a,s)$$<br/>
      <p>Finally, we obtain the <strong>Bellman Optimality Equation</strong> for <strong>the action-value function</strong></p>
      $$q^*(a,s)=R_s^a + \gamma\sum_{s'}P_{ss'}^a\max_{a'} q^*(a',s')$$

      <h3 id="How-to-solve-the-Bellman-equation?">How to solve the Bellman equation?</h3><ol>
      <li>The Bellman Optimality equation is non-linear and it is therefore difficult to compute a closed-form solution.</li>
      <li>Using (i) <strong>dynamic programming</strong>, (ii) <strong>Q-learning</strong> (iii) <strong>Sarsa</strong>.</li>
      </ol>

      <h3 id="Extensions-of-MDP">Extensions of MDP</h3><ol>
      <li>Infinite and continuous MDPs (continuous actions and/or continuous states)</li>
      <li>POMDPs<blockquote><p>A <strong>POMDP</strong> is a tuple $&lt;S,A,O,P,R,Z,\gamma&gt;$, where</p>
      <ul>
      <li>$\dots$</li>
      <li>$O$ is a finite set of observations</li>
      <li>$Z$ is the observation probability, namely $Z_{s',o}^a=P(O_{t+1}=o|S_{t+1}=s',A_t=a)$</li>
      </ul>
      </blockquote>
      </li>
      <li>Undiscounted, average reward MDP ($\gamma=1$)</li>
      </ol>
      <h2 id="Visualization-Summary-of-Different-Processes">Visualization Summary of Different Processes</h2><p><img src="./img/Summary.png" alt="Summary"></p>
      <h1 id="Appendix:-Probability-Review">Appendix: Probability Review</h1><p>Given the sample space $\Omega$ of a random quantity.</p>
      <blockquote><p><strong>Event</strong>. It is a subset of $\Omega$</p>
      <p><strong>Random variable</strong>. It is a valued-function defined on $\Omega$, namely $X: \Omega\rightarrow?$</p>
      <p><strong>Expectation</strong>. $E\{X\}=\sum_{\omega\in\Omega}X(\omega)P(\omega)$</p>
      <p><strong>Expectation conditioned on event (Conditional expectation)</strong>. $E\{X|E\}=\sum_{\omega\in E}X(\omega)P(\omega)$</p>
      </blockquote>
      
      

    </div><!-- /.article-wrap -->

    <div id="disqus_thread" style="width:140%;text-align:justify;margin-top:3cm;margin-bottom:2cm;"></div>
    <script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://ema87.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
    </script>
            
  </article>
</div><!-- /#index -->
                    

<div class="footer-wrap">
  <footer>
<span>&copy; 2020 Emanuele Sansone.

  </span></footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="https://emsansone.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="https://emsansone.github.io/assets/js/scripts.min.js"></script>

</body></html>
