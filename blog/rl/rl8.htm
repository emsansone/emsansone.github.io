<!DOCTYPE html>

<head>
<!-- saved from url=(0028) https://emsansone.github.io/ -->
<!-- <html class=" js no-touch rgba hsla textshadow opacity svg" lang="en"><!--<![endif]-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Emanuele Sansone</title>
<meta name="description" content="Emanuele Sansone">
<meta name="keywords" content="Jekyll, theme, responsive, blog, template">


<!-- Twitter Cards -->
<meta name="twitter:title" content="Emanuele Sansone">
<meta name="twitter:description" content="Emanuele Sansone">



<!--<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://emsansone.github.io/img/Background3.jpg">-->

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Emanuele Sansone">
<meta property="og:description" content="Emanuele Sansone">
<meta property="og:url" content="https://emsansone.github.io/blog/">
<meta property="og:site_name" content="Emanuele Sansone">





<link rel="canonical" href="https://emsansone.github.io/blog/">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="https://emsansone.github.io/assets/css/main.css">
    
<meta http-equiv="cleartype" content="on">
        
<!-- Modernizr -->
<script src="https://emsansone.github.io/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>
        
<link href='//fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700%7CPT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>


<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="https://emsansone.github.io/img/logo.png">
<!-- 32x32 -->
<link rel="shortcut icon" href="https://emsansone.github.io/img/logo-32x32.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="https://emsansone.github.io/img/logo-57x57.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://emsansone.github.io/img/logo-72x72.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://emsansone.github.io/img/logo-114x114.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://emsansone.github.io/img/logo-144x144.png">

<style>
  table, th, td {
    border: 1px solid grey;
    border-collapse: collapse;
  }
</style>

</head>

<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    // Center justify equations in code and markdown cells. Elsewhere
    // we use CSS to left justify single line equations in code cells.
    displayAlign: 'center',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}},
        linebreaks: { automatic: true }
    }
});
</script>

<body class="page">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="https://emsansone.github.io/">Website</a>
	</div><!-- /.site-name -->
    <div class="top-navigation">
        <nav role="navigation" id="site-nav" class="nav">
		    <ul>
              <li><a href="https://emsansone.github.io/blog/rl/rl0.htm">Index</a></li>	        					    
					    <li><a href="https://emsansone.github.io/blog/rl/rl7.htm" >Previous Lecture</a></li>
					    <li><a href="https://emsansone.github.io/blog/rl/rl9.htm" >Next Lecture</a></li>

		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->


<!--<div class="image-wrap">
    <img src="https://emsansone.github.io/img/Background3.jpg" alt="Emanuele Sansone feature image">
</div>-->

<div id="main" role="main">
  <div class="article-author-side">
    <img src="https://emsansone.github.io/img/Me.jpg" class="bio-photo" alt="Emanuele Sansone bio photo">

    <h2>Emanuele Sansone</h2>
    <p>PhD in machine learning and artificial intelligence.</p>
    <a href="mailto:e.sansone@hotmail.it" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/email.png" height=14px width=14px> Email</a>

    <a href="https://linkedin.com/in/emanuele-sansone-97329475" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/linkedin.png" height=14px width=14px> LinkedIn</a>

    <a href="https://twitter.com/skiera87" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/twitter.png" height=14px width=14px> Twitter</a>

    <!-- <a href="https://github.com/emsansone" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/github.png" height=14px width=14px> Github</a> -->
  </div>

  <article class="page">

    <div class="article-wrap" style="width:140%;text-align:justify;">
    
      <h1 id="Lecture-8:-Model-Based-RL">Model-Based RL</h1>
      <p>Here is the summary of the lecture</p><br/>
      <p style="text-align: center;"><img src="./img/Model.png" alt="Model-based"></p><br/>
      <p>The paradigm of model-based RL is to learn <strong>a parameterized model</strong> which can be easily trained through <strong>supervised learning</strong> and can be used to <strong>generate experience</strong>.
      There are two reasons to prefer model-based over model-free RL:</p>
      <ol>
      <li>Training a model is in general easier than learning an action-value function (in fact, it is easier to learn the reward for a state-action pair than learning the return for a state-action pair). Arguably, you can use policy gradient algorithms.</li>
      <li>If the learnt model is good, then you can sample a experience from it, thus learning more efficiently.</li>
      </ol>
      <p>Typically, we assume conditional independence between the reward and the next state, namely</p>
      $$P(R_{t+1},S_{t+1}|A_t,S_t)=P(R_{t+1}|A_t,S_t)P(S_{t+1}|A_t,S_t)$$<br/>
      <p>Therefore, the model approximates the two conditional densities.</p>
      $$\begin{align}
          P(R_{t+1}|A_t,S_t)&amp;\approx P(R_{t+1}|A_t,S_t,\eta_1) \\
          P(S_{t+1}|A_t,S_t)&amp;\approx P(S_{t+1}|A_t,S_t,\eta_2)
      \end{align}$$<br/>
      <p>Different models can be used, namely:</p>
      <ul>
      <li>Nonparametric table lookup, viz. store tuples $&lt;S_t,A_t,R_{t+1},S_{t+1}&gt;$</li>
      <li>Table lookup, viz. estimate $P_{ss'}^a$ and $R_s^a$ for all $s,a,s'$.</li>
      <li>Linear models</li>
      <li>Nonliner models</li>
      </ul>
      <p><strong>IT'S IMPORTANT TO LEARN A GOOD MODEL (PROBLEM OF LEARNING A MODEL)</strong>.
        Once we have a model, we can generate experience. But, how can we exploit the sampled experience? <strong>(PROBLEM OF USING SAMPLED EXPERIENCE)</strong></p>

      <h1 id="How-to-Use-Simulated-Experience">How to Use Simulated Experience</h1>
      <p>A natural way to exploit the sampled experience is to use model-free RL.
      Here, we provide the pseudo-code for the most-general algorithm</p>
      
      <pre><code>
      1. s &lt;- initial state
      2. Initialize Model M
      3. Initialize policy/value functions
      4. loop until termination
      5.    a &lt;- Act in the real world
      6.    r &lt;- get reward
      7.    s' &lt;- sample next state
      8.    Learn policy/value functions on real experience (s,a,r,s')
      9.    Update model M based on s,a,r,s'
      10    Sample experience from model M and learn policy/value functions on simulated experience (MODEL-FREE RL)
      11    s &lt;- s'
      </code></pre>
      <p>In the pseudo-code there are two ways to learn policy/value functions. We can use either real experience or simulated experience or both. Furthemore, we can use different strategies to sample simulated experience. All these degrees of freedom allow to specify a wide variety of RL algorithms. In the next subsections, we overview some of them.</p>

      <h1 id="Sample-Based-Planning">Sample-Based Planning</h1>
      <p>In sample-based planning <strong>we don't learn from real experience</strong> (no line 8 in the pseudo code).
      Furthermore, the sampling of experience from model M is performed in a <strong>random way</strong> (random states, random actions).</p>

      <h1 id="Dyna">Dyna</h1>
      <p>In Dyna, we learn from both <strong>real and simulated experience</strong>.
      Furthermore, the sampling of experience from model M is performed in a <strong>random way</strong> (random states, random actions from previous observed ones).
      Dyna-Q combines model-based and value-based RL (viz. using the action-value function).</p>

      <h1 id="Simulation-Based-Search">Simulation-Based Search</h1>
      <p>In simulation-based search we learn <strong>only from simulated experience</strong>.
      The main difference with respect to previous cases lies in the sampling of experience. <strong>Instead of sampling randomly, we condition on the current state and we perform a search for the most promising experiences</strong>.
      Practically speaking, the differences consists of (i) setting the initial state of each simulated experience to the current one instead of choosing it randomly and (ii) using a <strong>simulation policy</strong> instead of a random one, to guide the search.</p>
      <p><strong>THE PROBLEM OF USING SIMULATED EXPERIENCE, IN PARTICULAR THE PROBLEM OF SAMPLING, IS CONVERTED TO A SEARCH PROBLEM (IN A TREE)</strong>.</p>
      <p>In general, there are different ways to perform search. There are classical approaches, based on <strong>uninformed search</strong> (e.g. breadth-first search, depth-first search) or <strong>informed search</strong> (e.g. best-first search, $A^*$ search), and non classical ones, based for example on <strong>adversarial search</strong>. For more details, please see <a href="http://aima.cs.berkeley.edu/">the book of Russell and Norvig</a>. In this subsection, we focus on best-first search strategies.</p>

      <h2 id="Simple-Monte-Carlo-Search">Simple Monte-Carlo Search</h2>
      <p>In simple Monte-Carlo search, line 10 in the pseudo-code is specified as follows:</p>
      
      <pre><code>Given fixed simulation policy
      a.    for each action a
      b.        sample K episodes of experience using the fixed policy based on initial state s and action a
      c.        Q(s,a) &lt;- Monte-Carlo estimate on K episodes
      </code></pre>
      <p>Characteristics:</p>
      <ol>
      <li>Simulation policy is fixed</li>
      <li>The action-value function is estimated for all actions, but only for the current state</li>
      <li>At each iteration of the for loop (line 4 in the pseudo-code), the action-value function is re-estimated</li>
      </ol>

      <h2 id="Monte-Carlo-Tree-Search">Monte-Carlo Tree Search</h2>
      <p>Monte-Carlo tree search modifies the first two characteristics of simple Monte-Carlo search. The main idea is to incrementally <strong>update the simulation policy at each episode</strong>. Furthermore, the action-value function is estimated for <strong>a subset of states and actions</strong> (namely those ones close to the root).
      Line 10 in the pseudo-code is specified as follows:</p>
      
      <pre><code>
      a. Initialize tree by setting the root to current state
      b. for K
      b.    Selection: traverse tree using tree policy (e.g. epsilon-greedy using estimated action-value function) until reaching an unexplored action a (leaf)
      c.    Expansion: expand the tree with state s' sampled from model M (from last visited state s and unexplored action a)
      d.    Rollout: take actions using rollout policy (e.g. random policy) until termination of episode
      e.    Backup: update all state-action pairs involved in the episode contained in the tree using Monte-Carlo estimate
    </code></pre>
      <p>Characteristics:</p>
      <ol>
      <li>Simulation policy consists of tree and rollout policy (the idea is to behave randomly in new cases)</li>
      <li>Simulation policy changes at each episode</li>
      <li>The action-value function is estimated for a subset of actions and states (namely those ones contained in the tree)</li>
      <li>At each iteration of the for loop (line 4 in the pseudo-code), the action-value function is re-estimated</li>
      </ol>
      <p>The drawback of this method is that it is high variance.</p>

      <h2 id="TD-Search">TD Search</h2>
      <p>To reduce variance, you can update the action value function at each time step of each episode.
      <br>Characteristics:</p>
      <ol>
      <li>Simulation policy changes at each step of each episode</li>
      </ol>
      <p>The drawback is that it has higher bias. Therefore, we can use TD($\lambda$) search.</p>

      <h1 id="Dyna-2">Dyna-2</h1>
      <p>We learn from both <strong>real and simulated experience</strong>. In particular, we combine TD learning on real experience and TD($\lambda$) search on simulated experience. For more details, check <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.161.2015&amp;rep=rep1&amp;type=pdf">2008 paper</a>.</p>
      
      <h1 id="Summary">Summary</h1>
      <table>
      <thead><tr>
      <th style="text-align:center">Name</th>
      <th style="text-align:center">Learning from real experience</th>
      <th style="text-align:center">Sampling (initial state)</th>
      <th style="text-align:center">Sampling (simulation policy)</th>
      <th style="text-align:center">Update value/policy</th>
      <th style="text-align:center">Bias</th>
      <th style="text-align:center">Variance</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td style="text-align:center">Sample-based planning</td>
      <td style="text-align:center">No</td>
      <td style="text-align:center">Random</td>
      <td style="text-align:center">Random</td>
      <td style="text-align:center">Each real action</td>
      <td style="text-align:center">Low</td>
      <td style="text-align:center">Very High</td>
      </tr>
      <tr>
      <td style="text-align:center">Dyna</td>
      <td style="text-align:center">Yes</td>
      <td style="text-align:center">Random</td>
      <td style="text-align:center">Random</td>
      <td style="text-align:center">Each real action</td>
      <td style="text-align:center">Low</td>
      <td style="text-align:center">Very High</td>
      </tr>
      <tr>
      <td style="text-align:center">Simulation-based search (Simple Monte-Carlo)</td>
      <td style="text-align:center">No</td>
      <td style="text-align:center">Current state</td>
      <td style="text-align:center">Fixed simulation policy</td>
      <td style="text-align:center">Each real action</td>
      <td style="text-align:center">Low</td>
      <td style="text-align:center">High</td>
      </tr>
      <tr>
      <td style="text-align:center">Simulation-based search (Monte-Carlo Tree Search)</td>
      <td style="text-align:center">No</td>
      <td style="text-align:center">Current state</td>
      <td style="text-align:center">simulation policy (tree and rollout policies)</td>
      <td style="text-align:center">Each simulated episode</td>
      <td style="text-align:center">Low</td>
      <td style="text-align:center">Medium</td>
      </tr>
      <tr>
      <td style="text-align:center">Simulation-based search (TD Search)</td>
      <td style="text-align:center">No</td>
      <td style="text-align:center">Current state</td>
      <td style="text-align:center">simulation policy</td>
      <td style="text-align:center">Each step of simulated episode</td>
      <td style="text-align:center">Medium</td>
      <td style="text-align:center">Low</td>
      </tr>
      <tr>
      <td style="text-align:center">Simulation-based search (TD($\lambda$) Search)</td>
      <td style="text-align:center">No</td>
      <td style="text-align:center">Current state</td>
      <td style="text-align:center">simulation policy</td>
      <td style="text-align:center">Each step of simulated episode</td>
      <td style="text-align:center">Medium/Low (tradeoff)</td>
      <td style="text-align:center">Medium/Low (tradeoff)</td>
      </tr>
      <tr>
      <td style="text-align:center">Dyna-2</td>
      <td style="text-align:center">Yes</td>
      <td style="text-align:center">Current state</td>
      <td style="text-align:center">simulation policy (TD($\lambda$) Search)</td>
      <td style="text-align:center">Each real action</td>
      <td style="text-align:center">Medium/Low (tradeoff)</td>
      <td style="text-align:center">Medium/Low (tradeoff)</td>
      </tr>
      </tbody>
      </table>
      <p>For more updated reference papers about model-based RL check <a href="https://spinningup.openai.com/en/latest/spinningup/keypapers.html#model-based-rl">OPENAI website</a>.</p>
      
      
    </div><!-- /.article-wrap -->

    <div id="disqus_thread" style="width:140%;text-align:justify;margin-top:3cm;margin-bottom:2cm;"></div>
    <script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://ema87.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
    </script>
            
  </article>
</div><!-- /#index -->
                    

<div class="footer-wrap">
  <footer>
<span>&copy; 2020 Emanuele Sansone.

  </span></footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="https://emsansone.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="https://emsansone.github.io/assets/js/scripts.min.js"></script>

</body></html>
