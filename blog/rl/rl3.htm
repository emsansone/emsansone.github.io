<!DOCTYPE html>

<head>
<!-- saved from url=(0028) https://emsansone.github.io/ -->
<!-- <html class=" js no-touch rgba hsla textshadow opacity svg" lang="en"><!--<![endif]-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Emanuele Sansone</title>
<meta name="description" content="Emanuele Sansone">
<meta name="keywords" content="Jekyll, theme, responsive, blog, template">


<!-- Twitter Cards -->
<meta name="twitter:title" content="Emanuele Sansone">
<meta name="twitter:description" content="Emanuele Sansone">



<!--<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://emsansone.github.io/img/Background3.jpg">-->

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Emanuele Sansone">
<meta property="og:description" content="Emanuele Sansone">
<meta property="og:url" content="https://emsansone.github.io/blog/">
<meta property="og:site_name" content="Emanuele Sansone">





<link rel="canonical" href="https://emsansone.github.io/blog/">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="https://emsansone.github.io/assets/css/main.css">
    
<meta http-equiv="cleartype" content="on">
        
<!-- Modernizr -->
<script src="https://emsansone.github.io/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>
        
<link href='//fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700%7CPT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>


<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="https://emsansone.github.io/img/logo.png">
<!-- 32x32 -->
<link rel="shortcut icon" href="https://emsansone.github.io/img/logo-32x32.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="https://emsansone.github.io/img/logo-57x57.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://emsansone.github.io/img/logo-72x72.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://emsansone.github.io/img/logo-114x114.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://emsansone.github.io/img/logo-144x144.png">

<style>
  table, th, td {
    border: 1px solid grey;
    border-collapse: collapse;
  }
  </style>

</head>

<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    // Center justify equations in code and markdown cells. Elsewhere
    // we use CSS to left justify single line equations in code cells.
    displayAlign: 'center',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}},
        linebreaks: { automatic: true }
    }
});
</script>

<body class="page">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="https://emsansone.github.io/">Website</a>
	</div><!-- /.site-name -->
    <div class="top-navigation">
        <nav role="navigation" id="site-nav" class="nav">
		    <ul>
              <li><a href="https://emsansone.github.io/blog/rl/rl0.htm">Index</a></li>	        					    
					    <li><a href="https://emsansone.github.io/blog/rl/rl2.htm" >Previous Lecture</a></li>
					    <li><a href="https://emsansone.github.io/blog/rl/rl4.htm" >Next Lecture</a></li>

		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->


<!--<div class="image-wrap">
    <img src="https://emsansone.github.io/img/Background3.jpg" alt="Emanuele Sansone feature image">
</div>-->

<div id="main" role="main">
  <div class="article-author-side">
    <img src="https://emsansone.github.io/img/Me.jpg" class="bio-photo" alt="Emanuele Sansone bio photo">

    <h1>Emanuele Sansone</h1>
    <p>PhD in machine learning and artificial intelligence.</p>
    <a href="mailto:e.sansone@hotmail.it" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/email.png" height=14px width=14px> Email</a>

    <a href="https://linkedin.com/in/emanuele-sansone-97329475" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/linkedin.png" height=14px width=14px> LinkedIn</a>

    <a href="https://twitter.com/skiera87" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/twitter.png" height=14px width=14px> Twitter</a>

    <!-- <a href="https://github.com/emsansone" class="author-social" target="_blank"><img src="https://emsansone.github.io/img/github.png" height=14px width=14px> Github</a> -->
  </div>

  <article class="page">

    <div class="article-wrap" style="width:140%;text-align:justify;">
    
      <h1 id="Lecture-3:-Plannning-by-Dynamic-Programming">Plannning by Dynamic Programming</h1>
      <p>The <strong>planning</strong> problem consists of 2 subproblems:</p>
      <ol>
      <li>The <strong>prediction</strong> problem. Compute the state-value function under a given policy</li>
      <li>The <strong>control</strong> problem. Find the optimal policy</li>
      </ol>

      <h2 id="Prediction:-Iterative-Policy-Evaluation">Prediction: Iterative Policy Evaluation</h2>
      <p>The goal is to compute the <strong>state-value</strong> function. 
      Since the environment is known, we can compute it <strong>exactly</strong> through the Bellman Expectation Equation.
      Recall that in an MDP, the Bellman Expectation Equation is expressed as $$v_\pi=R^\pi + \gamma P^\pi v_\pi$$ 
      where</p>
      <ul>
      <li>$R_s^\pi=\sum_a \pi(a|s)R_s^a$.</li>
      <li>$P_{ss'}^\pi=\sum_a \pi(a|s)P_{ss'}^a$.</li>
      </ul>
      <br><strong>Solution</strong>. Apply <strong>iteratively</strong> $v_\pi^{k+1}=R^\pi + \gamma P^\pi v_\pi^{k}$, starting from random $v_\pi^{0}$. Note that this is a <strong>synchronous</strong> update (all states are updated simultaneously).</br>

      <br><strong>Correcteness</strong>. Iterative policy evaluation converges to true $v_\pi(s)$. Furthermore, it converges linearly.</br>
      <p><strong>Proof</strong>. Given two any value vectors $u,v\in\mathbb{R}^{|S|}$, we have that</p>
      $$\|u-v\|_{\infty}=\max_{s\in S}|u(s)-v(s)|$$<br/>
      <p>Define the Bellman Expectation operator $T^\pi(\cdot)=R^\pi + \gamma P^\pi \cdot$. Now note that,</p>

      <span class="math display">\[\begin{align}\|T^\pi(u)-T^\pi(v)\|_{\infty} &amp;= \|R^\pi + \gamma P^\pi u - R^\pi - \gamma P^\pi v\|_{\infty} \\
        &amp;= \|\gamma P^\pi u - \gamma P^\pi v\|_{\infty} \\
        &amp;= \gamma\|P^\pi(u - v)\|_{\infty} \\
        &amp;\leq \gamma\|P^\pi\|_{\infty}\|u - v\|_{\infty} \quad\text{in fact}\quad \|A\|_p\doteq\sup_{x\neq 0}\frac{\|Ax\|_p}{\|x\|_p}\geq \frac{\|Ax\|_p}{\|x\|_p} \quad\text{therefore}\quad \|Ax\|_p\leq\|A\|_p\|x\|_p \\
        &amp;\leq\gamma\|u - v\|_{\infty}  \quad\text{in fact}\quad \|P^\pi\|_\infty=\sup_{\|x\|_\infty=1}\|P^\pi x\|_\infty=\sup_{\|x\|_\infty=1}\max_i\sum_j P_{ij}^\pi x_j \leq \max_i\sum_j P_{ij}^\pi=1\end{align}\]</span>

      <p>Therefore, the Bellman Expectation operator is a <strong>$\gamma$-contraction mapping</strong>.</p>
      <p>Now, the contraction mapping theorem (Banach fixed-point theorem) says that the mapping converges to a unique point at linear convergence rate defined by $\gamma$. Therefore, this convergence point is the solution of the Bellman expectation equation. <strong>QED</strong></p>

      <h2 id="Control:-Policy-Iteration">Control: Policy Iteration</h2>
      <p>The goal is to compute an <strong>optimal deterministic policy</strong>.
      <br><strong>Solution</strong>. Iterative solution
      <ul>
      <li>Stage 1. Given policy $\pi$, compute state-value function using <strong>iterative policy evaluation</strong>.</li>
      <li>Stage 2. Get greedy deterministic policy $\pi'$, viz. <strong>search in the space of deterministic policies</strong>.</li>
      </ul></p>
      $$\pi'(s)=\arg\max_a q_\pi(a,s) \quad\text{ where }\quad q_\pi(a,s)=R_s^a+\gamma\sum_{s'}P_{ss'}^av_\pi(s')$$

      <br><strong>Correctness</strong>. At every iteration, stage 2 improves the policy. The algorithm converges to optimal policy.</br>
      <p><strong>Proof</strong>. First, note that the second stage searches over deterministic policy. Therefore, any candidate satisfies the following relation:</p>
      $$\begin{align}v_\pi(s) &amp;= \sum_a \pi(a|s)q_\pi(a,s) \\
                              &amp;= q_\pi(s,\pi(s)) \quad\text{ as the policy is deterministic}\end{align}$$<br/>
      <p>Now consider the policy chosen in stage 2 (i.e. policy $\pi'$ which chooses action $a'$).</p>
      $$q_\pi(s,\pi'(s))=q_\pi(s,a')=\max_a q_\pi(s,a) \geq q_\pi(s,a)=q_\pi(s,\pi(s))=v_\pi(s)$$<br/>
      <p>Therefore,</p>
      $$\begin{align}v_\pi(s) &amp;\leq q_\pi(s,\pi'(s)) \\
                              &amp;= E_{\pi'}\{R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s\} \quad\text{current action using }\pi'\text{, while others following }\pi \\
                              &amp;\leq E_{\pi'}\{R_{t+1}+\gamma q_\pi(S_{t+1},\pi'(S_{t+1}))|S_t=s\}  \quad\text{ as }\quad v_\pi(S_{t+1})\leq q_\pi(S_{t+1},\pi'(S_{t+1})) \\
                              &amp;= E_{\pi'}\{R_{t+1}+\gamma R_{t+2}+\gamma^2 v_\pi(S_{t+2})|S_t=s\}  \\
                              &amp;= \dots \\
                              &amp;\leq E_{\pi'}\{R_{t+1}+\gamma R_{t+2}+\dots|S_t=s\}=v_{\pi'}(s)\end{align}$$<br/>
                              
      <p>And this is valid for every $s$. <strong>QED [first part]</strong></p>
      <br>Once the algorithm stops, we have that $v_\pi(s)=v_{\pi'}(s)$ for all $s$.
      Now, since $v_\pi(s)=q_\pi(s,\pi(s))\leq q_\pi(s,\pi'(s)) \leq v_{\pi'}(s)$, we have that</br><br/>
      $$v_\pi(s) = q_\pi(s,\pi(s)) = q_\pi(s,\pi'(s))$$<br/>
      <p>But note that $q_\pi(s,\pi'(s)) = \max_a q_\pi(s,a)$. Therefore,</p>
      $$v_\pi(s) = \max_a q_\pi(s,a)$$<br/>
      <p>which is exactly the Bellman Optimality Equation. Consequently, this is the optimal solution. <strong>QED [second part]</strong>.</p>

      <h2 id="Control:-Modified-Policy-Iteration">Control: Modified Policy Iteration</h2>
      <p>Speeding up the algorithm of policy iteration, in particular <strong>speeding up policy evaluation</strong>.
      Two possibilities, both <strong>approximate convergence to</strong> $v_\pi$, namely:</p>
      <ol>
      <li>Stopping early <strong>$\epsilon$-convergence</strong>.</li>
      <li>Stopping after $k$ iterations of policy evaluation.</li>
      </ol>

      <h2 id="Control:-Value-Iteration">Control: Value Iteration</h2>
      <p>The main drawback with policy iteration is that the algorithm draws a <strong>sequence of policies</strong> and <strong>for each of them it runs policy evaluation</strong>. How can we be more efficient?
      <br>Let's imagine that we can access the optimal policy. 
      We know that the optimal policy has a state-value function that is solution of the <strong>Bellman Optimality Equation</strong>. Can we apply it iteratively?
      <br><br><strong>Solution</strong>. For every $s$, apply $v^{k+1}(s)=\max_a R_s^a + \gamma\sum_{s'}P_{ss'}^a v^{k}(s)$, starting from random $v^{0}(s)$. Note that this is a <strong>synchronous</strong> update (all states are updated simultaneously).<br><br>
      <strong>Correctness</strong>. The algorithm converges to optimal <strong>state-value</strong> function.
      <br><strong>Proof</strong>. Show that Bellman Optimality Equation is $\gamma$-contraction. Then, the proof is the same as the one used in Policy evaluation.
      <br><br><strong>Important Remark</strong>. Value iteration is equivalent to modified policy iteration with $k=1$. Therefore, it is much more efficient than policy iteration! Note also that the intermediate solutions obtained by the algorithm may not have a policy!</p>

      <h2 id="Overview-of-Dynamic-Programming-Algorithms">Overview of Dynamic Programming Algorithms</h2>
      <table>
      <thead><tr>
      <th style="text-align:left"><strong>Problem</strong></th>
      <th style="text-align:center"><strong>Bellman Equation</strong></th>
      <th style="text-align:left"><strong>Algorithm</strong></th>
      <th style="text-align:left"><strong>Time Complexity</strong></th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td style="text-align:left">Prediction</td>
      <td style="text-align:center">Expectation</td>
      <td style="text-align:left">Iterative Policy Evaluation</td>
      <td style="text-align:left">$O(mn^2)$</td>
      </tr>
      <tr>
      <td style="text-align:left">Control</td>
      <td style="text-align:center">Expectation + Greedy Policy Improvement</td>
      <td style="text-align:left">Policy Iteration</td>
      <td style="text-align:left">$O(mn^2)$</td>
      </tr>
      <tr>
      <td style="text-align:left">Control</td>
      <td style="text-align:center">Optimality</td>
      <td style="text-align:left">Value Iteration</td>
      <td style="text-align:left">$O(mn^2)$</td>
      </tr>
      </tbody>
      </table>
      <p>$m$ is the number of actions, $n$ is the number of states.<br>
      <strong>Problem. These solutions are not scalable in the number of states!</strong></p>

      <h2 id="Practical-Tips-for-Dynamic-Programming-Algorithms">Practical Tips for Dynamic Programming Algorithms</h2>
      <p><strong>Synchronous update requires to look and update each state at each iteration, but this is inefficient</strong>.
      You can use <strong>asynchronous updates as long as you pick all states</strong>.</p>

      <h3 id="In-Place-Dynamic-Programming">In-Place Dynamic Programming</h3>
      <p>In synchronous dynamic programming, you have <strong>two state-value functions</strong> and one iteration is given by the following code</p>
      <blockquote><p>Code.</p>
      <ul>
      <li>for all s in S</li>
      <li>$v_{new}(s) = \max_a R_s^a + \gamma\sum_{s'}P_{ss'}^av_{old}(s')$</li>
      <li>end for</li>
      <li>$v_{old} = v_{new}$</li>
      </ul>
      </blockquote>
      <p>But note that the Bellman Optimality Equation is a contraction mapping and <strong>convergence is guaranteed independently from the starting point</strong>. Therefore, we can use <strong>in-place operations</strong></p>
      <blockquote><p>Code.</p>
      <ul>
      <li>for all s in S</li>
      <li>$v(s) = \max_a R_s^a + \gamma\sum_{s'}P_{ss'}^av(s')$</li>
      <li>end for</li>
      </ul>
      </blockquote>

      <h3 id="Prioritized-Sweeping">Prioritized Sweeping</h3>
      <p>Note that the for loop in the in-place DP runs the update of the states in the <strong>same order</strong>. Nevertheless, we can improve this by sweeping the order and <strong>choose first the states which make larger updates</strong>. This can be implemented by using a priority queue (namely a heap) based on the <strong>Bellman error</strong></p>
      $$\Big|\max_a\big(R_s^a + \gamma\sum_{s'}P_{ss'}^av(s')\big)-v(s)\Big|$$

      <h3 id="Real-time-Dynamic-Programming">Real-time Dynamic Programming</h3>
      <p>Use only the state that are relevant to the agent $\tilde{S}\subseteq S$</p>

      <h2 id="Open-Problem">Open Problem</h2>
      <p><strong>Dynamic programming requires the knowledge of $R$ and $P$. How can we get rid of it?</strong>
      See next lecture (model-free RL).</p>
            

    </div><!-- /.article-wrap -->

    <div id="disqus_thread" style="width:140%;text-align:justify;margin-top:3cm;margin-bottom:2cm;"></div>
    <script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://ema87.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
    </script>
            
  </article>
</div><!-- /#index -->
                    

<div class="footer-wrap">
  <footer>
<span>&copy; 2020 Emanuele Sansone.

  </span></footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="https://emsansone.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="https://emsansone.github.io/assets/js/scripts.min.js"></script>

</body></html>
